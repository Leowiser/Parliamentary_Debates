base_url = "https://www.theyworkforyou.com"
# Initialize a data frame to store the results
results_df <- data.frame(
speech_id = character(),
debate_date = character(),
debate_title = character (),
speech_content = character(),
speaker_name = character(),
speaker_party = character(),
stringsAsFactors = FALSE
)
# Initialize vectors to store all titles and links
all_titles <- c()
all_links <- c()
# Assuming SearchTerms, base_url, StartDate are already defined in your environment
for (i in seq_along(SearchTerms)) {
link <- paste0(base_url, "/search/?q=", SearchTerms[i], "&phrase=&exclude=&from=", StartDate, "&to=", EndDate,"&person=&section=debates&column=")
print(link)
Sys.sleep(sample(3:4, 1))  # Random delay between 3 to 5 seconds
# Error handling for the main page
page <- tryCatch({
read_html(link)
}, error = function(e) {
message("Error accessing main search page: ", link)
return(NULL)
})
if (is.null(page)) next  # Skip to the next iteration if the page is NULL
# Extract titles and links from the first page
titles <- page %>%
html_nodes(".search-result__title a") %>%  # Adjust the CSS selector if needed
html_text()
links <- page %>%
html_nodes(".search-result__title a") %>%  # Adjust the CSS selector if needed
html_attr("href")
# Combine them with all_titles and all_links
all_titles <- c(all_titles, titles)
all_links <- c(all_links, links)
# Get the final page number
final_page_number <- page %>%
html_nodes(".search-result-pagination a[title='Final page']") %>%
html_attr("href") %>%
str_extract("p=\\d+") %>%
str_remove("p=") %>%
as.numeric()
# Loop through all subsequent pages and extract titles and links
if (!is.na(final_page_number) && length(final_page_number) > 0&& final_page_number > 1) {
for (j in 2:final_page_number) {
next_link <- paste0(base_url, "/search/?q=", SearchTerms[i], "&from=", StartDate, "&to=", EndDate, "&p=", j)
print(next_link)
Sys.sleep(sample(3:4, 1))  # Random delay between 3 to 5 seconds
# Error handling for subsequent pages
next_page <- tryCatch({
read_html(next_link)
}, error = function(e) {
message("Error accessing subsequent search page: ", next_link)
return(NULL)
})
if (is.null(next_page)) next  # Skip to the next iteration if the page is NULL
# Extract titles and links from the current page
titles <- next_page %>%
html_nodes(".search-result__title a") %>%
html_text()
links <- next_page %>%
html_nodes(".search-result__title a") %>%
html_attr("href")
# Append them to all_titles and all_links
all_titles <- c(all_titles, titles)
all_links <- c(all_links, links)
}
} else {
print("Only one page exists or final page number not found.")
}
# Filter links to only include ones with the search term, European, Europe which are not
# Written answers.
filtered_links <- all_links[
grepl(paste(TitleTerms[i], "European", "Europe", sep="|"), all_titles, ignore.case = TRUE) &
!grepl("Written Answer", all_titles, ignore.case = TRUE)
]
print(filtered_links)
if (length(filtered_links) > 0){
for (link_debate in filtered_links){
Sys.sleep(sample(3:4,1))  # Random delay between 3 to 5 seconds
# Find debate pages
debate_page <- tryCatch({
read_html(paste0(base_url,link_debate))
}, error = function(e) {
message("Error accessing debate page: ", link_debate)
return(NULL)
})
if (is.null(debate_page)) next  # Skip to the next iteration if the page is NULL
# Extract debate date
debate_date <- debate_page %>%
html_node("p.lead a") %>%
html_text(trim = TRUE)
# Extract the debate title
debate_title <- debate_page %>%
html_node("div.debate-header__content h1") %>%
html_text(trim = TRUE)
# Extract debate speeches
speeches <- debate_page %>%
html_nodes(".debate-speech")
# Loop through each speech node to extract the required information
for (speech in speeches) {
# Get the speech ID
speech_id <- speech %>%
html_attr("id")
# Get the speaker's name
speaker_name <- speech %>%
html_node(".debate-speech__speaker .debate-speech__speaker__name") %>%
html_text(trim = TRUE)
# Get the speaker's position (if it exists)
speaker_party <- speech %>%
html_node(".debate-speech__speaker .debate-speech__speaker__position") %>%
html_text(trim = TRUE)
# Get the speech content
speech_content <- speech %>%
html_node(".debate-speech__content") %>%
html_text(trim = TRUE)
results_df <- results_df %>%
add_row(
speech_id = speech_id,  # Character type
debate_date = debate_date,  # Character type
debate_title = debate_title,  # Character type
speech_content = speech_content,  # Character type
speaker_name = speaker_name,  # Character type
speaker_party = speaker_party  # Character type
)
}
}
} else{
print("No debate found for the specific topics.")
}
}
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
setwd("C:/Users/LENOVO/Desktop/iskola stuff/mester stuff/Collecting and Analyzing Big Data for Social Sciences/Group Project")
#### Data cleaning ####
df_result = read.csv('Parliamentary_Debates.csv', row.names = 1)
View(df_result)
# Delete all the duplicates
df_clean <- distinct(df_result)
n_distinct(df_clean$speech_id)
n_distinct(df_clean$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
#### Simple sentiment analysis ####
poswords = "https://cssbook.net/d/positive.txt"
negwords = "https://cssbook.net/d/negative.txt"
pos = scan(poswords, what = "list")
neg = scan(negwords, what = "list")
sentimentdict = dictionary(list(pos = pos, neg = neg))
# Need to rename the column of the speeches to 'text' because the corpus() function from the following code needs that naming convention
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Match and calculate the sentiments
sentiment_speech = df_clean %>%
corpus() %>%
tokens() %>%
dfm() %>%
dfm_lookup(sentimentdict) %>%
convert(to = "data.frame") %>%
mutate(sent = pos-neg)
df_clean <- cbind(df_clean, sentiment_speech[,-1])
df_clean %>%
group_by(debate_title) %>%
summarise(mean_sent = mean(sent)) %>%
ggplot(aes(x = mean_sent, y = debate_title)) +
geom_bar(stat = "identity", fill = 'steelblue') +
theme_minimal()
# This is probably incorrect, would need grouping like before imo
ggplot(df_clean, aes(x = debate_date, y = sent)) +
geom_line(color='steelblue') +
xlab("Year") +
theme_minimal()
### POS tagging ####
library(udpipe)
# load text
text <- readLines("https://slcladal.github.io/data/testcorpus/linguistics06.txt", skipNul = T)
# clean data
text <- text %>%
str_squish()
# Download english model - enough to do once
m_eng <- udpipe_download_model(language = "english-ewt")
# Get the path of the downloaded model
m_eng_path <- m_eng$file_model
# Load the english model from the path --> no need to download every time
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
# tokenise, tag, dependency parsing
text_anndf <- udpipe_annotate(m_eng_loaded, x = text) %>%
as.data.frame() %>%
select(-sentence)
# inspect
head(text_anndf, 10)
start_time <- Sys.time()
test <- udpipe_annotate(m_eng_loaded, x = df_clean$text) %>%
as.data.frame()
end_time <- Sys.time()
end_time-start_time
View(test)
# So I will use UDPipe --> test df
# Need to map the POS tags (upos) to the 5 POS tags in the lexicon --> DOUBLE CHECK WITH WHOLE, BIGGER DATASET!!! (might be more POS tags than here)
unique(test$upos)
View(test)
View(test)
negating_words <- c("not", "no", "never", "neither", "nor", "less")
negating_words <- c("not", "no", "never", "neither", "nor", "less")
save(test)
a <- Sys.time()
test %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
mutate(valence = ifelse(lag(lemma) %in% negating_words & lead(upos) == "PUNCT", 1, 0)) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) -> test_filtered
b <- Sys.time()
b-a
# Load the lexicon from the paper  trained on parliamentary speeches
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
sentiment_lexicon %>%
# Mapping
mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
pos1 == 'v' ~ 'VERB',
pos1 == 'a' ~ 'ADJ',
pos1 == 'r' ~ 'ADV',
pos1 == 'u' ~ 'INTJ')) %>%
# Remove inrelevant column
select(-seed) -> sentiment_lexicon
a <- Sys.time()
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>% View()
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
a <- Sys.time()
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence))
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
# Load the lexicon from the paper  trained on parliamentary speeches
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
sentiment_lexicon %>%
# Mapping
mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
pos1 == 'v' ~ 'VERB',
pos1 == 'a' ~ 'ADJ',
pos1 == 'r' ~ 'ADV',
pos1 == 'u' ~ 'INTJ')) %>%
# Remove inrelevant column
select(-seed) -> sentiment_lexicon
a <- Sys.time()
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence))
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
View(test_filtered)
View(sentiment_speech)
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>% View()
a <- Sys.time()
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence))
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
View(sentiment_lexicon)
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>% view()
a <- Sys.time()
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>%
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
b <- Sys.time()
b-a
View(sentiments_by_speech)
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') -> df_clean
# Visualize
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
ggplot(aes(x = debate_date, y = average_sentiment)) +
geom_line(color='steelblue') +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
save.image("C:/Users/LENOVO/Downloads/asd.RData")
load("C:/Users/LENOVO/Downloads/asd.RData")
# Visualize
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
ggplot(aes(x = debate_date, y = average_sentiment)) +
geom_line(color='steelblue') +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
View(df_clean)
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)) %>% View()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) -> proba
View(proba)
View(test)
View(sentiment_speech)
View(test)
View(sentiments_by_speech)
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>% View()
View(df_clean)
View(sentiments_by_speech)
View(test_filtered)
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) %>% View()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) %>% View()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>% View()
# Delete all the duplicates
df_clean <- distinct(df_result)
n_distinct(df_clean$speech_id)
n_distinct(df_clean$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
#### Simple sentiment analysis ####
poswords = "https://cssbook.net/d/positive.txt"
negwords = "https://cssbook.net/d/negative.txt"
pos = scan(poswords, what = "list")
neg = scan(negwords, what = "list")
sentimentdict = dictionary(list(pos = pos, neg = neg))
# Need to rename the column of the speeches to 'text' because the corpus() function from the following code needs that naming convention
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Match and calculate the sentiments
sentiment_speech = df_clean %>%
corpus() %>%
tokens() %>%
dfm() %>%
dfm_lookup(sentimentdict) %>%
convert(to = "data.frame") %>%
mutate(sent = pos-neg)
df_clean <- cbind(df_clean, sentiment_speech[,-1])
df_clean %>%
group_by(debate_title) %>%
summarise(mean_sent = mean(sent)) %>%
ggplot(aes(x = mean_sent, y = debate_title)) +
geom_bar(stat = "identity", fill = 'steelblue') +
theme_minimal()
# This is probably incorrect, would need grouping like before imo
ggplot(df_clean, aes(x = debate_date, y = sent)) +
geom_line(color='steelblue') +
xlab("Year") +
theme_minimal()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>% View()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) %>% View()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>% view()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>% view()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>% view()
View(df_clean)
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>% view()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) %>% View()
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
# Visualize
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
ggplot(aes(x = debate_date, y = average_sentiment)) +
geom_line(color='steelblue') +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) -> proba
View(proba)
View(proba)
smooth.spline(proba$average_sentiment, spar = 0.5)
smooth.spline(proba$average_sentiment, spar = 0.5)$
y
smooth.spline(proba$average_sentiment, spar = 0.5)$y
smooth.spline(proba$average_sentiment, spar = 0.5)$x
smooth.spline(proba$average_sentiment, spar = 0.5)$y
asdasd<- smooth.spline(proba$average_sentiment, spar = 0.5)
asdasd
View(asdasd)
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate((sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y)) %>% view()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y) %>% view()
# Visualize
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y) %>% view()
mblue <- "#002A48"
pblue <- "#3F5F75"
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y) %>% view()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y) %>% view()
save.image("C:/Users/LENOVO/Downloads/sentiment_workspace.RData")
