library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
nrow(df_result)
n_distinct(df_result)
nrow(df_result)
nrow(df_result) == n_distinct(df_result)
df_result = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
df_clean$debate_date <- dmy(df_result$debate_date)
df_debates = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Chacking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_debates$debate_date <- dmy(df_debates$debate_date)
View(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
c(n_distinct(df_debates$speech_id), n_distinct(df_debates$speech_content)))
c(n_distinct(df_debates$speech_id), n_distinct(df_debates$speech_content))
debates =
df_debates = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Checking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
View(debates)
View(df_debates)
df_debates = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Checking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
# Delete empty (NA) speeches
df_clean <- df_debates[complete.cases(df_debates$speech_content), ]
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library(udpipe)    # for POS tagging
library(spacyr)    # for POS tagging
library(writexl)    # to write xlsx
df_debates = read.csv('Parliamentary_Debates_Combined.csv', row.names = 1)
# Checking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
# Delete empty (NA) speeches
df_clean <- df_debates[complete.cases(df_debates$speech_content), ]
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
# A later function needs the speeches to be in a column called 'text' se we rename it
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Load the lexicon from Rheault et al. 2016
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
# Remove irrelevant column
sentiment_lexicon %>% select(-seed) -> sentiment_lexicon
head(sentiment_lexicon)
# Load the POS tagged data and model
load("pos.RData")
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
# Sample 100 speeches
set.seed(123)    #For reproducibility
sampled_speeches <- df_clean %>% sample_n(100)
# Lemmatize and POS tag with UDPipe
pos_validation_UDPipe <- udpipe_annotate(m_eng_loaded, x = sampled_speeches$text) %>%
as.data.frame()
# Lemmatize and POS tag with Spacyr
spacy_initialize(model = "en_core_web_sm")
pos_validation_spacy <- spacy_parse(sampled_speeches$text, lemma = TRUE, pos = TRUE, tag = TRUE)
# Save files for easier manual validation in Excel
pos_validation_UDPipe %>% write_xlsx("pos_validation_UDPipe.xlsx")
pos_validation_spacy %>% write_xlsx("pos_validation_spacy.xlsx")
# Mapping POS tags
sentiment_lexicon %>%
mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
pos1 == 'v' ~ 'VERB',
pos1 == 'a' ~ 'ADJ',
pos1 == 'r' ~ 'ADV',
pos1 == 'u' ~ 'INTJ')) -> sentiment_lexicon
# Create a lookup table that store the sentiments of each speech
negating_words <- c("not", "no", "never", "neither", "nor", "less")
pos %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
# Negation and punctuation indicator column
mutate(is_negation = lemma %in% negating_words,
is_punctuation = upos == "PUNCT") %>%
# Running flag to identify regions between punctuations and regions between negating words
mutate(punctuation_block = cumsum(is_punctuation),
negation_block = cumsum(is_negation)) %>%
# If there is a switch in negation block inside the same punctuation block we assign valence
group_by(punctuation_block, negation_block) %>%
mutate(valence = ifelse(any(is_negation), 0, 1)) %>%
ungroup() %>%
# Remove temporary auxuliary columns
select(-is_negation, -is_punctuation, -negation_block, -negation_block) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*valence) %>%
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
# Map the sentiments to the speeches in the original data df_clean
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
pos_validation_UDPipe %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
mutate(valence = ifelse(lag(lemma) %in% negating_words & lead(upos) == "PUNCT", 0, 1)) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*valence) -> sentiment_validation
# Save file for easier manual validation in Excel
sentiment_validation %>% write_xlsx("sentiment_validation.xlsx")
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"
# Visualize the sentiment by date
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
# Visualize the sentiment by date - standardized data, as in paper
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
# scale_y_continuous(limits = c(-3,3)) +
theme_minimal()
# Visualize the sentiment by quarter - standardized data, as in paper
df_clean %>%
mutate(quarter = paste0(year(debate_date), "-Q", quarter(debate_date))) %>%
group_by(quarter) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum) / sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
mutate(quarter = factor(quarter, levels = unique(quarter))) %>%
ggplot(aes(x = quarter, group = 1)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
xlab("Quarter") +
ylab('Emotional Polarity') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Visualize the sentiment by year - standardized data, as in paper
df_clean %>%
group_by(year) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum) / sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = year, group = 1)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
scale_x_continuous(breaks=c(2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022))  +
theme_minimal()
View(df_clean)
View(pos)
View(pos)
View(pos)
# Create a lookup table that store the sentiments of each speech
negating_words <- c("not", "no", "never", "neither", "nor", "less")
pos %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
# Negation and punctuation indicator column
mutate(is_negation = lemma %in% negating_words,
is_punctuation = upos == "PUNCT") %>%
# Running flag to identify regions between punctuations and regions between negating words
mutate(punctuation_block = cumsum(is_punctuation),
negation_block = cumsum(is_negation)) %>%
# If there is a switch in negation block inside the same punctuation block we assign valence
group_by(punctuation_block, negation_block) %>%
mutate(valence = ifelse(any(is_negation), 0, 1)) %>%
ungroup() %>%
# Remove temporary auxuliary columns
select(-is_negation, -is_punctuation, -negation_block, -negation_block) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*valence) %>%
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment) & sentiment != 0)) -> sentiments_by_speech
# Map the sentiments to the speeches in the original data df_clean
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library(udpipe)    # for POS tagging
library(spacyr)    # for POS tagging
library(writexl)    # to write xlsx
df_debates = read.csv('Parliamentary_Debates_Combined.csv', row.names = 1)
# Checking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
# Delete empty (NA) speeches
df_clean <- df_debates[complete.cases(df_debates$speech_content), ]
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
# A later function needs the speeches to be in a column called 'text' se we rename it
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Load the lexicon from Rheault et al. 2016
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
# Remove irrelevant column
sentiment_lexicon %>% select(-seed) -> sentiment_lexicon
head(sentiment_lexicon)
# Load the POS tagged data and model
load("pos.RData")
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
# Sample 100 speeches
set.seed(123)    #For reproducibility
sampled_speeches <- df_clean %>% sample_n(100)
# Lemmatize and POS tag with UDPipe
pos_validation_UDPipe <- udpipe_annotate(m_eng_loaded, x = sampled_speeches$text) %>%
as.data.frame()
# Lemmatize and POS tag with Spacyr
spacy_initialize(model = "en_core_web_sm")
pos_validation_spacy <- spacy_parse(sampled_speeches$text, lemma = TRUE, pos = TRUE, tag = TRUE)
# Save files for easier manual validation in Excel
pos_validation_UDPipe %>% write_xlsx("pos_validation_UDPipe.xlsx")
pos_validation_spacy %>% write_xlsx("pos_validation_spacy.xlsx")
unique(pos$upos)
unique(sentiment_lexicon$pos1)
# Mapping POS tags
sentiment_lexicon %>%
mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
pos1 == 'v' ~ 'VERB',
pos1 == 'a' ~ 'ADJ',
pos1 == 'r' ~ 'ADV',
pos1 == 'u' ~ 'INTJ')) -> sentiment_lexicon
# Create a lookup table that store the sentiments of each speech
negating_words <- c("not", "no", "never", "neither", "nor", "less")
pos %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
# Negation and punctuation indicator column
mutate(is_negation = lemma %in% negating_words,
is_punctuation = upos == "PUNCT") %>%
# Running flag to identify regions between punctuations and regions between negating words
mutate(punctuation_block = cumsum(is_punctuation),
negation_block = cumsum(is_negation)) %>%
# If there is a switch in negation block inside the same punctuation block we assign valence
group_by(punctuation_block, negation_block) %>%
mutate(valence = ifelse(any(is_negation), 0, 1)) %>%
ungroup() %>%
# Remove temporary auxuliary columns
select(-is_negation, -is_punctuation, -negation_block, -negation_block) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*valence) %>%
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment) & sentiment != 0)) -> sentiments_by_speech
# Map the sentiments to the speeches in the original data df_clean
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
pos_validation_UDPipe %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
mutate(valence = ifelse(lag(lemma) %in% negating_words & lead(upos) == "PUNCT", 0, 1)) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*valence) -> sentiment_validation
# Save file for easier manual validation in Excel
sentiment_validation %>% write_xlsx("sentiment_validation.xlsx")
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"
# Visualize the sentiment by date
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
# Visualize the sentiment by date - standardized data, as in paper
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
# scale_y_continuous(limits = c(-3,3)) +
theme_minimal()
# Visualize the sentiment by quarter - standardized data, as in paper
df_clean %>%
mutate(quarter = paste0(year(debate_date), "-Q", quarter(debate_date))) %>%
group_by(quarter) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum) / sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
mutate(quarter = factor(quarter, levels = unique(quarter))) %>%
ggplot(aes(x = quarter, group = 1)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
xlab("Quarter") +
ylab('Emotional Polarity') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Visualize the sentiment by year - standardized data, as in paper
df_clean %>%
group_by(year) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum) / sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = year, group = 1)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
scale_x_continuous(breaks=c(2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022))  +
theme_minimal()
# Load the lexicon from Rheault et al. (2016)
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
# Remove irrelevant column
sentiment_lexicon %>% select(-seed) -> sentiment_lexicon
knitr::kable(head(sentiment_lexicon), caption = "Observations form the lexicon of Rheault et al. (2016)")
View(pos)
View(sentiment_lexicon)
View(sentiment_validation)
