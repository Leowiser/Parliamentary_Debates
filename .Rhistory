anova(fit_no_ici,fit_aic)
### point 4
## time/covariate interaction
# cannot use a factor in the following function, must define a binary covariate
melanoma$ECEL_2 = (melanoma$ecel==2)
melanoma$ULC_1 = (melanoma$ulc==1)
melanoma$SEX_1 = (melanoma$sex==1)
# The following models test the statistical significance of a time-by-covariate
# interaction for each of the covariates in our model
# (look at the p-values for the tt() variable)
# see reference in report to see why 3 years was chosen as the cutoff value
fit_time_X_ecel <- coxph(Surv(days, dc)~ECEL_2+ULC_1+thick+SEX_1+age + tt(ECEL_2),
data=melanoma, tt = function(x,t,...) x * (t >= 365.25*3))
fit_time_X_ecel
fit_time_X_ulc <- coxph(Surv(days, dc)~ECEL_2+ULC_1+thick+SEX_1+age + tt(ULC_1),
data=melanoma,tt = function(x,t,...) x * (t >= 365.25*3))
fit_time_X_ulc
fit_time_X_thick <- coxph(Surv(days, dc)~ECEL_2+ULC_1+thick+SEX_1+age + tt(thick),
data=melanoma, tt = function(x,t,...) x * (t >= 365.25*3))
fit_time_X_thick
fit_time_X_sex <- coxph(Surv(days, dc)~ECEL_2+ULC_1+thick+SEX_1+age + tt(SEX_1),
data=melanoma, tt = function(x,t,...) x * (t >= 365.25*3))
fit_time_X_sex
fit_time_X_age <- coxph(Surv(days, dc)~ECEL_2+ULC_1+thick+SEX_1+age + tt(age),
data=melanoma, tt = function(x,t,...) x * (t >= 365.25*3))
fit_time_X_age
## Schoenfeld residuals
par(mfrow=c(1,2))
fit_schoenfeld <- cox.zph(fit_no_ici)
fit_schoenfeld
plot(fit_schoenfeld[1])
plot(fit_schoenfeld[2])
plot(fit_schoenfeld[3])
plot(fit_schoenfeld[4])
plot(fit_schoenfeld[5])
#### Part C/b) ####
# fit models assuming different distributions for T, then compare their AIC values
lognorm <- survreg(Surv(days, dc) ~ ecel + ulc + thick + sex + age,
data=melanoma, dist="lognormal")
weib <- survreg(Surv(days, dc) ~ecel + ulc + thick + sex + age,
data=melanoma, dist="weibull")
expon <- survreg(Surv(days, dc) ~ ecel + ulc + thick + sex + age,
data=melanoma, dist="exponential")
loglogist <- survreg(Surv(days, dc) ~ ecel + ulc + thick + sex + age,
data=melanoma, dist="loglogistic")
AIC <- c(extractAIC(lognorm)[2],extractAIC(weib)[2],extractAIC(expon)[2],
extractAIC(loglogist)[2])
names(AIC)<-c("log(normal)","weibull","exponential","log(logistic)")
AIC
# Coefficients and confidence bands for the best performing model (log-linear model)
summary(lognorm)
estimate <- coef(lognorm)
ci <- confint(lognorm)
cbind(estimate,ci)
# Invert sign of the coefficients to obtain AFT model -> gamma = - theta)
cbind(-estimate,-ci[,2],-ci[,1])
ggplot(melanoma, aes(x = ecel, y = days, fill = factor(dc), group = interaction(ecel, dc))) +
geom_boxplot(position = position_dodge(width = 0.75)) +
labs(x = "ecel", y = "Value", fill = "dc") +
facet_wrap(~dc) +
theme_minimal()
ggplot(melanoma, aes(x = ecel, y = days, fill = dc, group = interaction(ecel, dc))) +
geom_boxplot(position = position_dodge(width = 0.75)) +
labs(x = "ecel", y = "Value", fill = "dc") +
facet_wrap(~dc) +
theme_minimal()
ggplot(melanoma, aes(x = ecel, y = days, fill = factor(dc), group = interaction(ecel, dc))) +
geom_boxplot(position = position_dodge(width = 0.75)) +
labs(x = "ecel", y = "Value", fill = "dc") +
facet_wrap(~dc) +
theme_minimal()
ggplot(melanoma, aes(x = ecel, y = days, fill = factor(dc), group = interaction(ecel, dc))) +
geom_boxplot(position = position_dodge(width = 0.75)) +
labs(x = "ecel", y = "Value", fill = "dc") +
# facet_wrap(~dc) +
theme_minimal()
# Boxplots by 'ecel'
ggplot(melanoma, aes(x = ecel, y = days, fill = ecel)) +
geom_boxplot() +
labs(x = "Presence of epithelioid cells", y = "Days from operation") +
scale_fill_brewer(palette = "Blues") +
theme_minimal()+
theme(legend.position = "top")
ggplot(melanoma, aes(x = ecel, y = days, fill = factor(dc), group = interaction(ecel, dc))) +
geom_boxplot(position = position_dodge(width = 0.75)) +
labs(x = "ecel", y = "Value", fill = "dc") +
facet_wrap(~dc) +
theme_minimal()
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
setwd("C:/Users/LENOVO/Desktop/iskola stuff/mester stuff/Collecting and Analyzing Big Data for Social Sciences/Group Project")
#### Data cleaning ####
df_result = read.csv('Parliamentary_Debates_Combined.csv', row.names = 1)
# Delete all the duplicates
df_clean <- distinct(df_result)
n_distinct(df_clean$speech_id)
n_distinct(df_clean$speech_content)
View(df_clean)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
View(df_clean)
View(df_result)
#### Simple sentiment analysis ####
poswords = "https://cssbook.net/d/positive.txt"
negwords = "https://cssbook.net/d/negative.txt"
pos = scan(poswords, what = "list")
neg = scan(negwords, what = "list")
sentimentdict = dictionary(list(pos = pos, neg = neg))
# Need to rename the column of the speeches to 'text' because the corpus() function from the following code needs that naming convention
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Match and calculate the sentiments
sentiment_speech = df_clean %>%
corpus() %>%
tokens() %>%
dfm() %>%
dfm_lookup(sentimentdict) %>%
convert(to = "data.frame") %>%
mutate(sent = pos-neg)
df_clean <- cbind(df_clean, sentiment_speech[,-1])
df_clean %>%
group_by(debate_title) %>%
summarise(mean_sent = mean(sent)) %>%
ggplot(aes(x = mean_sent, y = debate_title)) +
geom_bar(stat = "identity", fill = 'steelblue') +
theme_minimal()
# This is probably incorrect, would need grouping like before imo
ggplot(df_clean, aes(x = debate_date, y = sent)) +
geom_line(color='steelblue') +
xlab("Year") +
theme_minimal()
View(df_clean)
View(df_result)
# Delete rows that have no speech (NA-s)
df_clean <- df_clean[complete.cases(df_clean$text), ]
View(df_result)
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
setwd("C:/Users/LENOVO/Desktop/iskola stuff/mester stuff/Collecting and Analyzing Big Data for Social Sciences/Group Project")
#### Data cleaning ####
df_result = read.csv('Parliamentary_Debates_Combined.csv', row.names = 1)
# Delete all the duplicates
df_clean <- distinct(df_result)
n_distinct(df_clean$speech_id)
n_distinct(df_clean$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
# Delete rows that have no speech (NA-s)
df_clean <- df_clean[complete.cases(df_clean$speech_content), ]
#### Simple sentiment analysis ####
poswords = "https://cssbook.net/d/positive.txt"
negwords = "https://cssbook.net/d/negative.txt"
pos = scan(poswords, what = "list")
neg = scan(negwords, what = "list")
sentimentdict = dictionary(list(pos = pos, neg = neg))
# Need to rename the column of the speeches to 'text' because the corpus() function from the following code needs that naming convention
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Match and calculate the sentiments
sentiment_speech = df_clean %>%
corpus() %>%
tokens() %>%
dfm() %>%
dfm_lookup(sentimentdict) %>%
convert(to = "data.frame") %>%
mutate(sent = pos-neg)
df_clean <- cbind(df_clean, sentiment_speech[,-1])
df_clean %>%
group_by(debate_title) %>%
summarise(mean_sent = mean(sent)) %>%
ggplot(aes(x = mean_sent, y = debate_title)) +
geom_bar(stat = "identity", fill = 'steelblue') +
theme_minimal()
View(df_clean)
# This is probably incorrect, would need grouping like before imo
ggplot(df_clean, aes(x = debate_date, y = sent)) +
geom_line(color='steelblue') +
xlab("Year") +
theme_minimal()
### POS tagging ####
library(udpipe)
# load text
text <- readLines("https://slcladal.github.io/data/testcorpus/linguistics06.txt", skipNul = T)
# clean data
text <- text %>%
str_squish()
# Download english model - enough to do once
m_eng <- udpipe_download_model(language = "english-ewt")
# Get the path of the downloaded model
m_eng_path <- m_eng$file_model
# Load the english model from the path --> no need to download every time
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
# tokenise, tag, dependency parsing
text_anndf <- udpipe_annotate(m_eng_loaded, x = text) %>%
as.data.frame() %>%
select(-sentence)
# inspect
head(text_anndf, 10)
# POS tagging with real data
start_time <- Sys.time()
start_time <- Sys.time()
test <- udpipe_annotate(m_eng_loaded, x = df_clean$text) %>%
as.data.frame()
end_time <- Sys.time()
end_time-start_time
save.image("C:/Users/LENOVO/Downloads/sentiment_workspace_new.RData")
# Compare it with Spacy - the previous (UDPipe) is better, e.g. because of 'co-operation'. But sometimes space is better e.g. 'collaborating'
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
a <- Sys.time()
test2 <- spacy_parse(df_clean$text, lemma = TRUE, pos = TRUE, tag = TRUE)
b <- Sys.time()
# So I will use UDPipe --> test df
# Need to map the POS tags (upos) to the 5 POS tags in the lexicon --> DOUBLE CHECK WITH WHOLE, BIGGER DATASET!!! (might be more POS tags than here)
unique(test$upos)
b-a
negating_words <- c("not", "no", "never", "neither", "nor", "less")
View(test)
test %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
mutate(valence = ifelse(lag(lemma) %in% negating_words & lead(upos) == "PUNCT", 1, 0)) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) -> test_filtered
# Load the lexicon from the paper  trained on parliamentary speeches
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
sentiment_lexicon %>%
# Mapping
mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
pos1 == 'v' ~ 'VERB',
pos1 == 'a' ~ 'ADJ',
pos1 == 'r' ~ 'ADV',
pos1 == 'u' ~ 'INTJ')) %>%
# Remove inrelevant column
select(-seed) -> sentiment_lexicon
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>%
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment))) -> sentiments_by_speech
View(sentiments_by_speech)
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
View(df_clean)
View(df_clean)
View(test)
# Visualize
mblue <- "#002A48"
pblue <- "#3F5F75"
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(proba$average_sentiment, spar = 0.5)$y) %>% view()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>% view()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = sum(sentiment_sum)/sum(sentiment_count)) %>% view()
View(df_clean)
View(sentiment_speech)
View(sentiments_by_speech)
test_filtered %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*(1-valence)) %>% view()
View(df_clean)
0/0
# Visualize
mblue <- "#002A48"
pblue <- "#3F5F75"
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
load("C:/Users/LENOVO/Downloads/sentiment_workspace.RData")
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0, sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Emotional Polarity') +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
df_result = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Delete all the duplicates
df_clean <- distinct(df_result)
n_distinct(df_clean$speech_id)
n_distinct(df_clean$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
n_distinct(df_clean)
n_distinct(df_clean) == len(df_clean)
n_distinct(df_clean) == length(df_clean)
length((df_clean))
nrow(df_clean)
str(df_clean$debate_date)
types(df_result$debate_date)
typeof(df_result$debate_date)
str(df_result$debate_date)
typeof(df_clean$debate_date)
str(typeof(df_result$debate_date))
str(df_result$debate_date)
str(df_clean$debate_date)
typeof(df_clean$debate_date)
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
nrow(df_result)
n_distinct(df_result)
nrow(df_result)
nrow(df_result) == n_distinct(df_result)
df_result = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
df_clean$debate_date <- dmy(df_result$debate_date)
df_debates = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Chacking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_debates$debate_date <- dmy(df_debates$debate_date)
View(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
c(n_distinct(df_debates$speech_id), n_distinct(df_debates$speech_content)))
c(n_distinct(df_debates$speech_id), n_distinct(df_debates$speech_content))
debates =
df_debates = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Checking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
View(debates)
View(df_debates)
df_debates = read.csv('Parliamentary_Debates_combined.csv', row.names = 1)
# Checking for duplicates
nrow(df_debates)
nrow(df_debates) == n_distinct(df_debates)
n_distinct(df_debates$speech_id)
n_distinct(df_debates$speech_content)
# Delete empty (NA) speeches
df_clean <- df_debates[complete.cases(df_debates$speech_content), ]
