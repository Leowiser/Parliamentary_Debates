library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library(udpipe)    # for POS tagging
library(spacyr)    # for POS tagging
library(writexl)    # to write xlsx
# Hand coded data frame based on the names gathered from the table before
new_party = read_excel("New party.xlsx")
# Based on the previous data frame combine the names with the assigned party.
# There are no names liste twice.
df_clean <- df_clean %>%
left_join(new_party, by = "speaker_name", suffix = c("", ".new")) %>%
mutate(Party = coalesce(Party.new, Party)) %>%
select(-Party.new)
print(table(df_clean$Party))
# Delete the phrase "I beg to move" as it is mandatory at the beginning of every debate
df_clean$speech_content <- sub("^I beg to move","",df_clean$speech_content)
# List of parties in the parliament during the 10 years
# as wel as other important positions
Parties <- c("Liberal Democrat", "Liberal Democrats", "Independent",
"Labour/Co-operative", "Co-operative","Labour", "Green",
"DUP", "Speaker", "Scottish National Party", "Conservative",
"Secretary","Shadow Minister", "Minister", "Plaid Cymru")
# Initialize a new column to store the matching substring(s)
df_clean$Party <- NA
# Loop through the search list and check for each substring, with case insensitivity
for (search_string in Parties) {
# Creates a regex pattern to make the match more flexible. Ex. Liberal Democrats should be catched as Liberal Democrat.
pattern <- paste0("\\b", search_string, "s?\\b")
# Adds the substring to the new column if it's found in the Text column
df_clean$Party <- ifelse(grepl(pattern, df_clean$speaker_party, ignore.case = TRUE),
ifelse(is.na(df_clean$Party), search_string,
paste(df_clean$Party, search_string, sep = ", ")),
df_clean$Party)
}
# Check number and exact description of the remaining parties
print(paste0("The new number of distinct party values: ",n_distinct(df_clean$Party)))
print(unique(df_clean$Party))
df_clean$Party <- gsub("Co-operative, Shadow Minister, Minister", "Co-operative", df_clean$Party)
df_clean$Party <- gsub("Green, Shadow Minister, Minister", "Green", df_clean$Party)
df_clean$Party <- gsub("Conservative, Minister", "Conservative", df_clean$Party)
df_clean$Party <- gsub("Labour, Secretary", "Labour", df_clean$Party)
df_clean$Party <- gsub("Labour, Secretary, Shadow Minister, Minister", "Labour", df_clean$Party)
df_clean$Party <- gsub("Labour/Co-operative, Co-operative, Labour", "Labour/Co-operative", df_clean$Party)
df_clean$Party <- gsub("Liberal Democrat, Liberal Democrats", "Liberal Democrat", df_clean$Party)
df_clean$Party <- gsub("Labour, Shadow Minister, Minister", "Labour", df_clean$Party)
df_clean$Party <- gsub("Independent, Speaker", "Independent", df_clean$Party)
unique(df_clean$speaker_name[df_clean$Party=="Secretary, Shadow Minister, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Shadow Minister, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Secretary, Shadow Minister, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Secretary, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Speaker, Secretary"])
unique(df_clean$speaker_name[df_clean$Party=="Green, Conservative"])
unique(df_clean$speaker_name[df_clean$Party=="Labour, Green"])
unique(df_clean$speaker_name[df_clean$Party=="Liberal Democrat, DUP"])
unique(df_clean$speaker_name[df_clean$Party=="Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Secretary"])
# Hand coded data frame based on the names gathered from the table before
new_party = read_excel("New party.xlsx")
# Based on the previous data frame combine the names with the assigned party.
# There are no names liste twice.
df_clean <- df_clean %>%
left_join(new_party, by = "speaker_name", suffix = c("", ".new")) %>%
mutate(Party = coalesce(Party.new, Party)) %>%
select(-Party.new)
print(table(df_clean$Party))
# Delete the phrase "I beg to move" as it is mandatory at the beginning of every debate
df_clean$speech_content <- sub("^I beg to move","",df_clean$speech_content)
# Delete the phrase "I beg to move" as it is mandatory at the beginning of every debate
df_clean$speech_content <- sub("^I beg to move","",df_clean$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
# A later function needs the speeches to be in a column called 'text' so we rename it
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Add a new column indicating whether the party was in the opposition or government at that time.
df_clean$Government_Opposition <- ifelse(
(df_clean$Party == "Conservative" & df_clean$debate_date <= as.Date("2015-05-08")) |
(df_clean$Party == "Conservative" & df_clean$debate_date > as.Date("2015-05-08")
& df_clean$debate_date < as.Date("2019-12-16")) |
(df_clean$Party == "DUP" & df_clean$debate_date > as.Date("2015-05-08")
& df_clean$debate_date < as.Date("2019-12-16")),
"Government",
ifelse(
df_clean$Party == "Conservative" & df_clean$debate_date >= as.Date("2019-12-16"),
"Government",
"Opposition"
)
)
custom_stopwords <- c("hon", "minister", "friend", "want", "can", "prime", "beg",
"parliament", "house", "commons")
tidy_speeches <- df_clean %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% custom_stopwords)
tidy_speeches <- df_clean %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% custom_stopwords)
word_counts <- tidy_speeches %>%
count(year, Government_Opposition, word, sort = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library(udpipe)    # for POS tagging
library(spacyr)    # for POS tagging
library(writexl)    # to write xlsx
#############################
###     Data Cleaning     ###
#############################
# Load the web scraped data
df_Debates = read.csv("Parliamentary_Debates_Combined.csv", row.names = 1)
# Delete all duplicated rows
df_clean <- distinct(df_Debates)
# Delete all rows where no speaker name exists as these are often not real speeches
# but the descriptions of the following debates
df_clean <- df_clean[!is.na(df_clean$speaker_name),]
# We will now try to find reoccurring speech contents to discard
# sentences that are following parliamental protocol but have no real meaning.
# However, they could effect our sentiment analysis.
df_value_counts <- df_clean %>%
group_by(speech_content) %>%
summarize(count = n()) %>%
filter(count > 4)
# Found sentences that occur often and should be filtered out as they are part of the
# parliamentary culture.
filter_list <- c("rose—","On a point of order, Mr Speaker.","indicated assent.",
"Will the Secretary of State give way?","indicated dissent.",
"Will my right hon. Friend give way?","Will the hon. Lady give way?",
"Will my hon. Friend give way?","Will the right hon. Gentleman give way?",
"Will the Minister give way?","Will the hon. Gentleman give way?",
"Will my right hon. and learned Friend give way?","Order.",
"Will the hon. and learned Lady give way?","Will the right hon. Lady give way?",
"I beg to move, That the clause be read a Second time.","rose —",
"Will the Prime Minister give way?","Will my hon. and learned Friend give way?",
"Will the Attorney General give way?","I will.",
"Will the hon. Gentleman give way on that point?","Will the Solicitor General give way?",
"Will the hon. and learned Gentleman give way?","I give way to my hon. Friend.",
"Will the Minister give way on that point?","I will give way to my hon. Friend.",
"Will my hon. Friend give way on that point?","Will the Foreign Secretary give way?",
"Will the hon. Lady give way on that point?","rose—[Interruption.]","I give way.",
"I will give way one more time.","I give way to the hon. Gentleman.",
"I will give way one last time.")
df_clean <- df_clean %>% filter(!speech_content %in% filter_list)
# clean the party column
# Check the current number of distinct values for the party
print(paste0("The current number of distinct party values: ",n_distinct(df_clean$speaker_party)))
# List of parties in the parliament during the 10 years
# as wel as other important positions
Parties <- c("Liberal Democrat", "Liberal Democrats", "Independent",
"Labour/Co-operative", "Co-operative","Labour", "Green",
"DUP", "Speaker", "Scottish National Party", "Conservative",
"Secretary","Shadow Minister", "Minister", "Plaid Cymru")
# Initialize a new column to store the matching substring(s)
df_clean$Party <- NA
# Loop through the search list and check for each substring, with case insensitivity
for (search_string in Parties) {
# Creates a regex pattern to make the match more flexible. Ex. Liberal Democrats should be catched as Liberal Democrat.
pattern <- paste0("\\b", search_string, "s?\\b")
# Adds the substring to the new column if it's found in the Text column
df_clean$Party <- ifelse(grepl(pattern, df_clean$speaker_party, ignore.case = TRUE),
ifelse(is.na(df_clean$Party), search_string,
paste(df_clean$Party, search_string, sep = ", ")),
df_clean$Party)
}
# Check number and exact description of the remaining parties
print(paste0("The new number of distinct party values: ",n_distinct(df_clean$Party)))
print(unique(df_clean$Party))
df_clean$Party <- gsub("Co-operative, Shadow Minister, Minister", "Co-operative", df_clean$Party)
df_clean$Party <- gsub("Green, Shadow Minister, Minister", "Green", df_clean$Party)
df_clean$Party <- gsub("Conservative, Minister", "Conservative", df_clean$Party)
df_clean$Party <- gsub("Labour, Secretary", "Labour", df_clean$Party)
df_clean$Party <- gsub("Labour, Secretary, Shadow Minister, Minister", "Labour", df_clean$Party)
df_clean$Party <- gsub("Labour/Co-operative, Co-operative, Labour", "Labour/Co-operative", df_clean$Party)
df_clean$Party <- gsub("Liberal Democrat, Liberal Democrats", "Liberal Democrat", df_clean$Party)
df_clean$Party <- gsub("Labour, Shadow Minister, Minister", "Labour", df_clean$Party)
df_clean$Party <- gsub("Independent, Speaker", "Independent", df_clean$Party)
unique(df_clean$speaker_name[df_clean$Party=="Secretary, Shadow Minister, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Shadow Minister, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Secretary, Shadow Minister, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Secretary, Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Speaker, Secretary"])
unique(df_clean$speaker_name[df_clean$Party=="Green, Conservative"])
unique(df_clean$speaker_name[df_clean$Party=="Labour, Green"])
unique(df_clean$speaker_name[df_clean$Party=="Liberal Democrat, DUP"])
unique(df_clean$speaker_name[df_clean$Party=="Minister"])
unique(df_clean$speaker_name[df_clean$Party=="Secretary"])
# Hand coded data frame based on the names gathered from the table before
new_party = read_excel("New party.xlsx")
# Based on the previous data frame combine the names with the assigned party.
# There are no names liste twice.
df_clean <- df_clean %>%
left_join(new_party, by = "speaker_name", suffix = c("", ".new")) %>%
mutate(Party = coalesce(Party.new, Party)) %>%
select(-Party.new)
knitr::kable(table(df_clean$Party))
# Delete the phrase "I beg to move" as it is mandatory at the beginning of every debate
df_clean$speech_content <- sub("^I beg to move","",df_clean$speech_content)
# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
# A later function needs the speeches to be in a column called 'text' so we rename it
names(df_clean)[names(df_clean) == "speech_content"] <- "text"
# Add a new column indicating whether the party was in the opposition or government at that time.
df_clean$Government_Opposition <- ifelse(
(df_clean$Party == "Conservative" & df_clean$debate_date <= as.Date("2015-05-08")) |
(df_clean$Party == "Conservative" & df_clean$debate_date > as.Date("2015-05-08")
& df_clean$debate_date < as.Date("2019-12-16")) |
(df_clean$Party == "DUP" & df_clean$debate_date > as.Date("2015-05-08")
& df_clean$debate_date < as.Date("2019-12-16")),
"Government",
ifelse(
df_clean$Party == "Conservative" & df_clean$debate_date >= as.Date("2019-12-16"),
"Government",
"Opposition"
)
)
print(n_distinct(df_clean$debate_date, df_clean$debate_title, na.rm = TRUE))
number_debates <- df_clean %>%
group_by(year) %>%
summarize(distinct_combinations = n_distinct(debate_date, debate_title))
knitr::kable(number_debates, caption = "Number of debates per year")
number_speakers <- df_clean %>%
group_by(year) %>%
summarize(distinct_combinations = n_distinct(speaker_name))
knitr::kable(number_speakers, caption = "Number of speakers per year")
custom_stopwords <- c("hon", "minister", "friend", "want", "can", "prime", "beg",
"parliament", "house", "commons")
tidy_speeches <- df_clean %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% custom_stopwords)
word_counts <- tidy_speeches %>%
count(year, Government_Opposition, word, sort = TRUE)
knitr::kable(sum(word_counts$n), caption = "Number of words")
total_words_by_year <- word_counts %>%
group_by(year) %>%
summarize(total_words = sum(n))
knitr::kable(total_words_by_year, caption = "Number of words per year")
top_words_combined <- word_counts %>%
group_by(year) %>%
top_n(15, n)
combined_plot <- ggplot(top_words_combined, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity", position = "dodge", fill = "orange") +
coord_flip() +
facet_wrap(~ year, scales = "free_y") +
labs(title = "Most Common Words by Year and Government/Opposition",
x = "Words",
y = "Frequency") +
theme_minimal()
print(combined_plot)
library(dplyr)
library(ggplot2)
filtered_words <- word_counts %>%
filter(!is.na(Government_Opposition)) %>%  # Change to "Opposition" if needed
group_by(Government_Opposition) %>%
top_n(15, n) %>%
ungroup()
Gov_plot <- ggplot(filtered_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity", position = "dodge", fill = "steelblue") +
coord_flip() +
facet_wrap(~ Government_Opposition, scales = "free_y") +
labs(title = "Most Common Words by Government/Opposition",
x = "Words",
y = "Frequency") +
theme_minimal()
print(Gov_plot)
# Load the lexicon from Rheault et al. (2016)
sentiment_lexicon <- read.csv("lexicon-polarity.csv")
# Remove irrelevant column
sentiment_lexicon %>% select(-seed) -> sentiment_lexicon
knitr::kable(head(sentiment_lexicon), caption = "Observations form the lexicon of Rheault et al. (2016)")
# Load the POS tagged data and model
load("pos.RData")
#spacy_install()
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")
# Sample 100 speeches
set.seed(123)    #For reproducibility
sampled_speeches <- df_clean %>% sample_n(100)
# Lemmatize and POS tag with UDPipe
pos_validation_UDPipe <- udpipe_annotate(m_eng_loaded, x = sampled_speeches$text) %>%
as.data.frame()
# Lemmatize and POS tag with Spacyr
#spacy_initialize(model = "en_core_web_sm")
#pos_validation_spacy <- spacy_parse(sampled_speeches$text, lemma = TRUE, pos = TRUE, tag = TRUE)
# Save files for easier manual validation in Excel
pos_validation_UDPipe %>% write_xlsx("pos_validation_UDPipe.xlsx")
#pos_validation_spacy %>% write_xlsx("pos_validation_spacy.xlsx")
unique(pos$upos)
unique(sentiment_lexicon$pos1)
# Mapping POS tags
sentiment_lexicon %>%
mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
pos1 == 'v' ~ 'VERB',
pos1 == 'a' ~ 'ADJ',
pos1 == 'r' ~ 'ADV',
pos1 == 'u' ~ 'INTJ')) -> sentiment_lexicon
# Create a lookup table that store the sentiments of each speech
negating_words <- c("not", "no", "never", "neither", "nor")
pos %>%
# Last 6 columns not needed
select(-tail(names(.), 6)) %>%
# Create 'valence' column as defined in the paper
# Negation and punctuation indicator column
mutate(is_negation = lemma %in% negating_words,
is_punctuation = upos == "PUNCT") %>%
# Running flag to identify regions between punctuations and regions between negating words
mutate(punctuation_block = cumsum(is_punctuation),
negation_block = cumsum(is_negation)) %>%
# If there is a switch in negation block inside the same punctuation block we assign valence
group_by(punctuation_block, negation_block) %>%
mutate(valence = ifelse(any(is_negation), 0, 1)) %>%
ungroup() %>%
# Remove temporary auxiliary columns
select(-is_negation, -is_punctuation, -negation_block, -negation_block) %>%
# Keep relevant POS tags only
filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
# Map polarity values
left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
# Calculate sentiment based on paper
mutate(sentiment = polarity*valence) %>%
# Calculate sum of sentiment for each speech and number of words that had a sentiment
group_by(doc_id) %>%
summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
sentiment_count = sum(!is.na(sentiment) & sentiment != 0)) -> sentiments_by_speech
# Map the sentiments to the speeches in the original data df_clean
df_clean %>%
# Add the doc_id column to df_clean to join on it in the subsequent step
mutate(doc_id = paste0('doc', row_number())) %>%
left_join(sentiments_by_speech, by = 'doc_id') %>%
# Replace NA values with 0 in the sentiment_sum and sentiment_count columns
mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"
# Visualize the sentiment by date
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Sentiment') +
scale_y_continuous(limits = c(0.15,0.62)) +
theme_minimal()
# Visualize the sentiment by date - standardized data, as in paper
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Sentiment') +
scale_y_continuous(limits = c(-3.5,4)) +
theme_minimal()
View(df_clean)
# Visualize the sentiment by quarter - standardized data, as in paper
df_clean %>%
mutate(quarter = paste0(year(debate_date), "-Q", quarter(debate_date))) %>%
group_by(quarter) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
sum(sentiment_sum) / sum(sentiment_count))) %>%
mutate(standardized_sentiment = scale(average_sentiment),
sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
mutate(quarter = factor(quarter, levels = unique(quarter))) %>%
ggplot(aes(x = quarter, group = 1)) +
geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
xlab("Quarter") +
ylab('Sentiment') +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"
# Visualize the sentiment by date
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Sentiment') +
scale_y_continuous(limits = c(0.15,0.62)) +
theme_minimal()
df_clean %>%
mutate(average_sentiment = ifelse(sentiment_count == 0, 0, sentiment_sum / sentiment_count)) %>%
mutate(period = case_when(
debate_date <= as.Date("2016-06-23") ~ "Before referendum",
debate_date >= as.Date("2016-06-24") & debate_date <= as.Date("2020-01-31") ~ "Between Brexit",
debate_date > as.Date("2020-01-31") ~ "After Brexit"
)) -> testasd
View(testasd)
pairwise_results <- pairwise.t.test(
x = testasd$average_sentiment,
g = testasd$period,
p.adjust.method = "bonferroni"
)
pairwise_results
testasd %>% group_by(period) %>%
summarise(average_of_averages = mean(average_sentiment, na.rm = TRUE)) %>% view()
anova_result <- aov(average_sentiment ~ period, data = testasd)
summary(anova_result)
TukeyHSD(anova_result)
# Average sentiments in different periods, testing difference
df_clean %>%
mutate(average_sentiment = ifelse(sentiment_count == 0, 0, sentiment_sum / sentiment_count)) %>%
mutate(period = case_when(
debate_date <= as.Date("2016-06-23") ~ "Before referendum",
debate_date >= as.Date("2016-06-24") & debate_date <= as.Date("2020-01-31") ~ "Between Brexit",
debate_date > as.Date("2020-01-31") ~ "After Brexit"
)) -> df_clean
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"
# Visualize the sentiment by date
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Sentiment') +
scale_y_continuous(limits = c(0.15,0.62)) +
theme_minimal()
# Average sentiments in different periods, testing difference
df_clean %>%
mutate(average_sentiment = ifelse(sentiment_count == 0, 0, sentiment_sum / sentiment_count)) %>%
mutate(period = case_when(
debate_date <= as.Date("2016-06-23") ~ "Before referendum",
debate_date >= as.Date("2016-06-24") & debate_date <= as.Date("2020-01-31") ~ "Between Brexit",
debate_date > as.Date("2020-01-31") ~ "After Brexit"
)) -> df_clean
df_clean %>% group_by(period) %>%
summarise(average_sents = mean(average_sentiment, na.rm = TRUE)) -> period_avgs
period_avgs
# ANOVA
anova <- aov(average_sentiment ~ period, data = df_clean)
summary(anova)
anova_result <- aov(average_sentiment ~ period, data = testasd)
summary(anova_result)
TukeyHSD(anova_result)
TukeyHSD(anova)
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"
# Visualize the sentiment by date
df_clean %>%
group_by(debate_date) %>%
summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
sum(sentiment_sum)/sum(sentiment_count))) %>%
mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
ggplot(aes(x = debate_date)) +
geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
xlab("Date") +
ylab('Sentiment') +
scale_y_continuous(limits = c(0.15,0.62)) +
theme_minimal()
# Average sentiments in different periods, testing difference
df_clean %>%
mutate(average_sentiment = ifelse(sentiment_count == 0, 0, sentiment_sum / sentiment_count)) %>%
mutate(period = case_when(
debate_date <= as.Date("2016-06-23") ~ "Before referendum",
debate_date >= as.Date("2016-06-24") & debate_date <= as.Date("2020-01-31") ~ "Between Brexit",
debate_date > as.Date("2020-01-31") ~ "After Brexit"
)) -> df_clean
df_clean %>% group_by(period) %>%
summarise(average_sents = mean(average_sentiment, na.rm = TRUE)) -> period_avgs
period_avgs
# ANOVA
anova <- aov(average_sentiment ~ period, data = df_clean)
summary(anova)
TukeyHSD(anova)
### Data collection
The data we gather include all debates between 25.10.2012 until the 25.10.2022. The time frame is chosen to cover 10 years and cover the years before and after the Referendum as well as official Brexit (31.01.2020). The data is constructed by searching for all debates in the time frame where either the European Union, the European Commission, the European Council, or Brexit was mentioned. These debates are then filtered to only include debates that have European Union, European Commission, European Council, Brexit, European, Europe, or EU (case insensitive) in their title and are no written answers. The web scraping algorithm covers 61 two month periods as previous tries showed that only up to three month time spans could be scraped reliably. Furthermore the date of the debate is saved as well as the name and party/position of the speakers.
