---
title: "Britain_on_Europe"
output:
  html_document:
    df_print: paged
  
date: "`r Sys.Date()`"
---

# The EU from a British perspective: 17 years of debates in the British parliament

## Introduction
The positive outcome of the British referendum to withdrawal from the EU in 2016 came as a surprise to most other member states of the European Union.With a wining margin of only 3.8% and an overall approval to leave the European Union by 51.9% the decision was conceivably close (House of Commons Library, 2016). This direct democratic approach was relatively unusual especially since the United Kingdom of England and Northern Ireland (UK) has a strong representative democracy in which the sovereignty of the parliament is "the most important part of the UK constitution" (UK Parliament)^[The parliament generally refers to the house of commons. Thus in the following text house of commons and parliament are used interchangeably]. Furthermore, the Prime Minister at the time David Cameron as well as the biggest opposition party very much favoured remaining in the EU and thus disagreeing with the outcome of the referendum (cf. p.445, Russell, 2020). This vast majority for remaining in the EU in the parliament did not only lead to "no proper planning for a Leave vote" (Public Administration and Constitutional Affairs Committee, 2017, p. 4), but also to a clear difference between what most members of parliament (MP) and what the population wanted. The struggle of how to leave the EU thus triggered nearly 4 years of debates in the house of commons.

Russell (2020) raises that these debates which overturned deals with the EU multiple times even lead to an anti-parliamentary rhetoric from the Prime Ministers Theresa May as well as Boris Johnson and he Attorney General Geoffrey Cox.

This anti-parliamentary populist rhetoric raises the question whether the UK parliament really had a strong positive sentiment towards the EU and even more interesting if that was especially strong during the times of the Brexit debates.

This clearly shows that the European Union and especially Brexit is a highly emotional topic in British politics. 

- Emotion shape plitics
- Rheault: Measuring Emotion in Parliamentary Debates with Automated Textual Analysis, shows that debates became more positive.

Thus the following paper tries to find answers to the following research question: 

* How did the sentiment in debates about EU topics changed over time in the house of commons?

* Is the sentiment significantly different during debates about Brexit or withdrawl from the EU?

* Is there a visible difference depending on the current majorities in parliament?

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data
To answer the question we need the data of the debates in the british parliament. To gather these parliamentary debates, we web scraped the the debates in the house of commons from the TheyWorkForYou website. TheyWorkForYou is run by mySociety a british non for profit organization and aims to make the work of the Uk parliament more accesible and transparent to the wider public (source). In this capacity they also publish a mirrored version of Hansard the official webpage for transcripts of UK debates (source). 

In our work we use TheyWorkForYou instead of Hansard as the content is exactly the same but Hansard uses CloudFlare which makes web scraping significantly more complicated.

### Data collection
The data we gather include all debates between 27.06.2007 until the 19.08.2024. The starting date is chosen as it is the first day of the Brown ministry the first government that had the possibility to retreat from the EU due to Article 50 (source) (more detailed explanation). The data is constructed by searching for all debates in the time frame that mention the European Union, European Commission, European Council, or Brexit. These debates are then filtered to only include debates that have European Union, European Commission, European Council, Brexit, European, and Europe in their title and are no written answers. Furthermore the date of the debate is saved as well as the name and party/position of the speakers in the debate are saved. Thus, resulting in a data frame representing 13,831 different debates.

```{r, eval=FALSE, echo=TRUE}
# Webscraping

###############################################################
###          THIS NEEDS AT LEAST 5 HOURS to RUN!!!          ### 
###############################################################

# Set working directory
setwd("C:/Users/LENOVO/Desktop/iskola stuff/mester stuff/Collecting and Analyzing Big Data for Social Sciences/Group Project")

# load necessary packages
# install.packages(c("tidyverse", "httr", "glue", "dplyr", "stringr"))
library(dplyr)
library("tidyverse")
library("httr")
library("rvest")
library("xml2")
library("glue")
library("rvest")
library("xml2")
library("glue")
library(stringr)
SearchTerm <- "(\"EU\"+OR+\"European+Commission\"+OR+\"European+Union\"+OR+\"European+Council\"+OR+\"Brexit\")"
TitleTerms <- c("European Union","European Commission","European Council", "Brexit")

StartDate <- c("2012-10-25", "2012-12-25", "2013-02-25", "2013-04-25", 
               "2013-06-25", "2013-08-25", "2013-10-25", "2013-12-25", 
               "2014-02-25", "2014-04-25", "2014-06-25", "2014-08-25", 
               "2014-10-25", "2014-12-25", "2015-02-25", "2015-04-25", 
               "2015-06-25", "2015-08-25", "2015-10-25", "2015-12-25", 
               "2016-02-25", "2016-04-25", "2016-06-25", "2016-08-25", 
               "2016-10-25", "2016-12-25", "2017-02-25", "2017-04-25", 
               "2017-06-25", "2017-08-25", "2017-10-25", "2017-12-25", 
               "2018-02-25", "2018-04-25", "2018-06-25", "2018-08-25", 
               "2018-10-25", "2018-12-25", "2019-02-25", "2019-04-25", 
               "2019-06-25", "2019-08-25", "2019-10-25", "2019-12-25", 
               "2020-02-25", "2020-04-25", "2020-06-25", "2020-08-25", 
               "2020-10-25", "2020-12-25", "2021-02-25", "2021-04-25", 
               "2021-06-25", "2021-08-25", "2021-10-25", "2021-12-25", 
               "2022-02-25", "2022-04-25", "2022-06-25", "2022-08-25", 
               "2022-10-25")

EndDate <- c("2012-12-25", "2013-02-25", "2013-04-25", "2013-06-25", 
             "2013-08-25", "2013-10-25", "2013-12-25", "2014-02-25", 
             "2014-04-25", "2014-06-25", "2014-08-25", "2014-10-25", 
             "2014-12-25", "2015-02-25", "2015-04-25", "2015-06-25", 
             "2015-08-25", "2015-10-25", "2015-12-25", "2016-02-25", 
             "2016-04-25", "2016-06-25", "2016-08-25", "2016-10-25", 
             "2016-12-25", "2017-02-25", "2017-04-25", "2017-06-25", 
             "2017-08-25", "2017-10-25", "2017-12-25", "2018-02-25", 
             "2018-04-25", "2018-06-25", "2018-08-25", "2018-10-25", 
             "2018-12-25", "2019-02-25", "2019-04-25", "2019-06-25", 
             "2019-08-25", "2019-10-25", "2019-12-25", "2020-02-25", 
             "2020-04-25", "2020-06-25", "2020-08-25", "2020-10-25", 
             "2020-12-25", "2021-02-25", "2021-04-25", "2021-06-25", 
             "2021-08-25", "2021-10-25", "2021-12-25", "2022-02-25", 
             "2022-04-25", "2022-06-25", "2022-08-25", "2022-10-25", 
             "2022-12-25")
base_url = "https://www.theyworkforyou.com"

for(i in 1:length(StartDate)){
  # Initialize a data frame to store the results
  results_df <- data.frame(
    speech_id = character(),
    debate_date = character(),
    debate_title = character (),
    speech_content = character(),
    speaker_name = character(),
    speaker_party = character(),
    stringsAsFactors = FALSE
  )
  
  # Initialize vectors to store all titles and links
  all_titles <- c()
  all_links <- c()
    
  link <- paste0(base_url, "/search/?q=%28%22EU%22+OR+%22European+Commission%22+OR+%22European+Union%22+OR+%22European+Council%22+OR+%22Brexit%22%29+",StartDate[i],"..",EndDate[i],"+section%3Adebates+section%3Awhall&p=1")
    
  # Error handling for the main page
  page <- tryCatch({
    read_html(link)
  }, error = function(e) {
    message("Error accessing main search page: ", link)
    return(NULL)
  })
    
  # Extract titles and links from the first page
  titles <- page %>%
    html_nodes(".search-result__title a") %>%  # Adjust the CSS selector if needed
    html_text()
  
  links <- page %>%
    html_nodes(".search-result__title a") %>%  # Adjust the CSS selector if needed
    html_attr("href")
    
  # Filter links that start with "/debates"
  debate_links <- links[grep("^/debates", links)]
  
  clean_link <- gsub("#.*$", "", links)
    
  # Combine them with all_titles and all_links
  all_titles <- c(all_titles, titles)
  all_links <- c(all_links, clean_link)

  # Get the final page number
  final_page_number <- page %>%
    html_nodes(".search-result-pagination a[title='Final page']") %>% 
    html_attr("href") %>% 
    str_extract("p=\\d+") %>% 
    str_remove("p=") %>% 
    as.numeric()
    
  # Loop through all subsequent pages and extract titles and links
  if (!is.na(final_page_number) && length(final_page_number) > 0&& final_page_number > 1) {
    for (j in 2:final_page_number) {
      next_link <- paste0(base_url, "/search/?q=%28%22EU%22+OR+%22European+Commission%22+OR+%22European+Union%22+OR+%22European+Council%22+OR+%22Brexit%22%29+",StartDate[i],"..",EndDate[i],"+section%3Adebates+section%3Awhall&p=", j)
        
      #Sys.sleep(sample(3:4, 1))  # Random delay between 3 to 5 seconds
        
      # Error handling for subsequent pages
      next_page <- tryCatch({
        read_html(next_link)
      }, error = function(e) {
        message("Error accessing subsequent search page: ", next_link)
        return(NULL)
      })
      
      if (is.null(next_page)) next  # Skip to the next iteration if the page is NULL
        
      # Extract titles and links from the current page
      titles <- next_page %>%
        html_nodes(".search-result__title a") %>%
        html_text()
        
      links <- next_page %>%
        html_nodes(".search-result__title a") %>%
        html_attr("href")
      
      # Filter links that start with "/debates"
      debate_links <- links[grep("^/debates", links)]
        
      clean_link <- gsub("#.*$", "", links)
        
      # Combine them with all_titles and all_links
      all_titles <- c(all_titles, titles)
      all_links <- c(all_links, clean_link)
    }
  } else {
    print("Only one page exists or final page number not found.")
  }
    
  # Filter links to only include ones with European, Europe, and EU in the title
  # which are not Written answers.
  filtered_links <- all_links[
    grepl(paste("European", "Europe", "EU", sep="|"), all_titles, ignore.case = TRUE) & 
      !grepl("Written Answer", all_titles, ignore.case = TRUE)
  ]
  
    for (link_debate in filtered_links){
      #Sys.sleep(sample(3:4,1))  # Random delay between 3 to 5 seconds
      
      # Find debate pages
      debate_page <- tryCatch({
        read_html(paste0(base_url,link_debate))
      }, error = function(e) {
        message("Error accessing debate page: ", link_debate)
        return(NULL)
      })
        
      if (is.null(debate_page)) next  # Skip to the next iteration if the page is NULL
      
      # Extract debate date
      debate_date <- debate_page %>%
        html_node("p.lead a") %>%
        html_text(trim = TRUE)
        
      # Extract the debate title
      debate_title <- debate_page %>%
        html_node("div.debate-header__content h1") %>%
        html_text(trim = TRUE)
        
      # Extract debate speeches
      speeches <- debate_page %>%
        html_nodes(".debate-speech")
        
      # Loop through each speech node to extract the required information
      for (speech in speeches) {
        # Get the speech ID
        speech_id <- speech %>% 
          html_attr("id")
        
        # Get the speaker's name
        speaker_name <- speech %>%
          html_node(".debate-speech__speaker .debate-speech__speaker__name") %>%
          html_text(trim = TRUE)
        
        # Get the speaker's position (if it exists)
        speaker_party <- speech %>%
          html_node(".debate-speech__speaker .debate-speech__speaker__position") %>%
          html_text(trim = TRUE)
          
        # Get the speech content
        speech_content <- speech %>%
          html_node(".debate-speech__content") %>%
          html_text(trim = TRUE)
          
        results_df <- results_df %>%
          add_row(
            speech_id = speech_id,  # Character type
            debate_date = debate_date,  # Character type
            debate_title = debate_title,  # Character type
            speech_content = speech_content,  # Character type
            speaker_name = speaker_name,  # Character type
            speaker_party = speaker_party  # Character type
          )
      }
    } 
  
  output_file <- file.path("C:/Users/leonw/OneDrive - KU Leuven/2nd Semester/Collecting Big Data for Social Science/Final Assignment", paste0("Parliamentary_Debates_", i, ".csv"))
  write.csv(results_df, output_file, row.names = FALSE)
  Sys.sleep(30)
}

csv_files <- list.files(pattern = "^Parliamentary_Debates_.*\\.csv$")

list_of_data_frames <- lapply(csv_files, read.csv)

# Step 4: Combine all data frames into one using do.call and rbind
combined_data_frame <- do.call(rbind, list_of_data_frames)

combined_data_frame$year <- as.integer(substr(combined_data_frame$debate_date, nchar(combined_data_frame$debate_date) - 4 + 1, nchar(combined_data_frame$debate_date)))

n_distinct(combined_data_frame$year)

df_clean <- distinct(combined_data_frame)

write.csv(df_clean, "Parliamentary_Debates_Combined.csv")
```

### Ethical considerations
Web scrapping websites always comes with certain ethical considerations. Checking the robots.txt of the websites shows that it is legal to scrape the debates in the house of commons. Despite the legal considerations sustainable approaches were used to scrape the data. First of all scraping was conducted during the night to avoid overloading the page in times when it is frequently used. Second a waiting time between 3 and 4 seconds was chosen before scraping to ensure not to burden the webpage to much.

## Data cleaning
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}

library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
library(udpipe)    # for POS tagging
library(spacyr)    # for POS tagging
library(writexl)    # to write xlsx
```
**THIS WAS LEFT HERE FROM ADAM - NEEDS TO CHANGE**
We do not have duplicates in the data as a whole, every row is unique. There are duplicate speeches though, the content of two speeches can be the same (e.g. if they were said by a different speaker or at a different time). Examples of such cases are "Will the Minister give way?", "Will the hon. Gentleman give way?", a simple "I will." or "No.", etc. - they are common expressions often said in parliamentary debates so the duplicates are not results of an error in the scraping or the structure of the data, and are valid. While checking the duplicate speeches we also notice some empty ones - since they carry no information regarding our analysis, we delete them. Finally a technicality, the dates of the debates were actually stored as character strings in R, but for our analysis we need them as dates, so we convert them. 

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
#############################
###     Data Cleaning     ### 
#############################

# Load the web scraped data
df_Debates = read.csv("Parliamentary_Debates_Combined.csv", row.names = 1)

# Delete all duplicated rows
df_clean <- distinct(df_Debates)

# Delete all rows where no speaker name exists as these are often not real speeches
# but the descriptions of the following debates
df_clean <- df_clean[!is.na(df_clean$speaker_name),]

# We will now try to find reoccurring speech contents to discard
# sentences that are following parliamental protocol but have no real meaning.
# However, they could effect our sentiment analysis.
df_value_counts <- df_clean %>%
  group_by(speech_content) %>%
  summarize(count = n()) %>%
  filter(count > 4)


# Found sentences that occur often and should be filtered out as they are part of the 
# parliamentary culture.
filter_list <- c("rose—","On a point of order, Mr Speaker.","indicated assent.",
                 "Will the Secretary of State give way?","indicated dissent.",
                 "Will my right hon. Friend give way?","Will the hon. Lady give way?",
                 "Will my hon. Friend give way?","Will the right hon. Gentleman give way?",
                 "Will the Minister give way?","Will the hon. Gentleman give way?",
                 "Will my right hon. and learned Friend give way?","Order.",
                 "Will the hon. and learned Lady give way?","Will the right hon. Lady give way?",
                 "I beg to move, That the clause be read a Second time.","rose —",
                 "Will the Prime Minister give way?","Will my hon. and learned Friend give way?",
                 "Will the Attorney General give way?","I will.",
                 "Will the hon. Gentleman give way on that point?","Will the Solicitor General give way?",
                 "Will the hon. and learned Gentleman give way?","I give way to my hon. Friend.",
                 "Will the Minister give way on that point?","I will give way to my hon. Friend.",
                 "Will my hon. Friend give way on that point?","Will the Foreign Secretary give way?",
                 "Will the hon. Lady give way on that point?","rose—[Interruption.]","I give way.",
                 "I will give way one more time.","I give way to the hon. Gentleman.",
                 "I will give way one last time.")

df_clean <- df_clean %>% filter(!speech_content %in% filter_list)

# clean the party column
# Check which parties exist
table(df_clean$speaker_party)

# List of parties in the parliament
Parties <- c("Liberal Democrat", "Liberal Democrats", "Independent", 
             "Labour/Co-operative", "Co-operative","Labour", "Green", 
             "DUP", "Speaker", "Scottish National Party", "Conservative", 
             "Secretary","Shadow Minister", "Minister", "Plaid Cymru")

# Initialize a new column to store the matching substring(s)
df_clean$Party <- NA

# Loop through the search list and check for each substring, with case insensitivity
for (search_string in Parties) {
  # Create a regex pattern to make the match more flexible
  pattern <- paste0("\\b", search_string, "s?\\b")
  
  # Add the substring to the new column if it's found in the Text column
  df_clean$Party <- ifelse(grepl(pattern, df_clean$speaker_party, ignore.case = TRUE), 
                              ifelse(is.na(df_clean$Party), search_string, 
                                     paste(df_clean$Party, search_string, sep = ", ")),
                              df_clean$Party)
}

table(df_clean$Party)

df_clean$Party <- gsub("Co-operative, Shadow Minister, Minister", "Co-operative", df_clean$Party)
df_clean$Party <- gsub("Green, Shadow Minister, Minister", "Green", df_clean$Party)
df_clean$Party <- gsub("Conservative, Minister", "Conservative", df_clean$Party)
df_clean$Party <- gsub("Labour, Secretary", "Labour", df_clean$Party)
df_clean$Party <- gsub("Labour, Secretary, Shadow Minister, Minister", "Labour", df_clean$Party)
df_clean$Party <- gsub("Labour/Co-operative, Co-operative, Labour", "Labour/Co-operative", df_clean$Party)
df_clean$Party <- gsub("Liberal Democrat, Liberal Democrats", "Liberal Democrat", df_clean$Party)
df_clean$Party <- gsub("Labour, Shadow Minister, Minister", "Labour", df_clean$Party)
df_clean$Party <- gsub("Independent, Speaker", "Independent", df_clean$Party)


table(df_clean$speaker_name[df_clean$Party=="Secretary, Shadow Minister, Minister"])
table(df_clean$speaker_name[df_clean$Party=="Shadow Minister, Minister"])
table(df_clean$speaker_name[df_clean$Party=="Secretary, Shadow Minister, Minister"])
table(df_clean$speaker_name[df_clean$Party=="Secretary, Minister"])
table(df_clean$speaker_name[df_clean$Party=="Speaker, Secretary"])
table(df_clean$speaker_name[df_clean$Party=="Green, Conservative"])
table(df_clean$speaker_name[df_clean$Party=="Labour, Green"])
table(df_clean$speaker_name[df_clean$Party=="Liberal Democrat, DUP"])
table(df_clean$speaker_name[df_clean$Party=="Minister"])
table(df_clean$speaker_name[df_clean$Party=="Secretary"])


# Hand coded data frame based on the names gathered from the table before
new_party = read_excel("New party.xlsx")

# Based on the previous data frame combine the names with the assigned party.
# There are no names liste twice.
df_clean <- df_clean %>%
  left_join(new_party, by = "speaker_name", suffix = c("", ".new")) %>%
  mutate(Party = coalesce(Party.new, Party)) %>%
  select(-Party.new)

table(df_clean$Party)

# Delete the phrase "I beg to move" as it is mandatory at the beginning of every debate
df_clean$speech_content <- sub("^I beg to move","",df_clean$speech_content)

# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)

# A later function needs the speeches to be in a column called 'text' se we rename it
names(df_clean)[names(df_clean) == "speech_content"] <- "text"

# Add a new column indicating whether the party was in the opposition or government at that time.
df_clean$Government_Opposition <- ifelse(
  (df_clean$Party == "Conservative" & df_clean$debate_date <= as.Date("2015-05-08")) |
    (df_clean$Party == "Conservative" & df_clean$debate_date > as.Date("2015-05-08")
     & df_clean$debate_date < as.Date("2019-12-16")) |
    (df_clean$Party == "DUP" & df_clean$debate_date > as.Date("2015-05-08")
     & df_clean$debate_date < as.Date("2019-12-16")),
  "Government", 
  ifelse(
    df_clean$Party == "Conservative" & df_clean$debate_date >= as.Date("2019-12-16"),
    "Government", 
    "Opposition"
  )
)
```

## Sentiments
Sentiment analysis is a technique used to determine the emotional tone (positive, negative or neutral) of a digital text, in our case parliamental speeches. For most cases, it is conducted by a rule-based or a machine learning approach. While the latter can be used to understand more complex and interesting concepts, it nowadays often utilizes deep learning and other complex methods that go beyond the concepts learned in the course and the purpose of this project. Thus, while we acknowledge that determining the sentiment of a text (speech) is also not a simple task, we will use the rule-based (or dictionary based) approach with a custom lexicon created from a corpus of parliamentary debates (**in UK etc.**) by Rheault et al. (2016). Using a domain-specific sentiment dictionary is important because lexicons attuned on different domains than the ones they are used for often perform poorly in sentiment analysis tasks.

We inspect the first few rows of the sentiment lexicon (available on the authors' [GitHub repository](https://github.com/lrheault/emotion?tab=readme-ov-file)) in Table (**reference**) and see that the sentiments are not simply assigned to words (to be more specific, lemmas) but to the combination of lemmas and their Part-of-Speech (POS) tag. While other popular dictionaries [**Maybe reference a few here? or later?**] often match sentiments only to the words, we see that this approach is already more sophisticated - Combining it with the fact that the lexicon was constructed from political texts, we can be hopeful that our method will capture the real sentiment of the speeches better. Rheault et al. (2016) refer to sentiment as "emotional polarity", hence the column 'polarity' in the table below, but they can be used interchangeably. The dictionary contains sentiments for 4200 words (lemmas) in the range of [-1, 1], -1 being most negative and 1 being most positive with an equal number of positive and negative sentiments (2100-2100). For more details about how the lexicon was constructed and polarities assigned, we refer to the original paper (**reference it again?**).
```{r sentiment-table, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
# Load the lexicon from Rheault et al. (2016)
sentiment_lexicon <- read.csv("lexicon-polarity.csv")

# Remove irrelevant column
sentiment_lexicon %>% select(-seed) -> sentiment_lexicon

knitr::kable(head(sentiment_lexicon), caption = "Observations form the lexicon of Rheault et al. (2016)")
```
Lemmatization is the process of finding the root (called lemma) of a word and is useful in computational linguistics because by analyzing various inflected forms of a word as a single item, text processing models can equate morphologically related words. POS tagging learns the linguistic category (noun/verb/adverb/etc.) of a word (called part of speech) and assigns it to the token. It is useful since the same word can have different meanings in different contexts (and thus, also different sentiments). In fact, the previously introduced dictionary we use indeed contains countless examples of the same lemma with different POS tags and sentiments.

Both methods require to understand the meaning of the words not just by themselves, but also within the larger context of the surrounding sentence, or even the whole document. This is not an easy task, and is often done by machine learning algorithms or nowadays deep learning. Our purpose with this project is not to train a lemmatizer and POS tagger model on our own corpora, so we will use a pre-trained one.

While stop word removal is a common pre-processing step in text analysis, they in fact help the model better understand the context of a word so we do not remove them and apply the pre-trained model on our raw texts (speeches). While several pre-trained models can be found on the internet, we will use UDPipe (Wijffels 2021) which seemed to be one of the most popular. To make sure it is adequate for our needs, we validate the lemmatization and POS tagging on a subset of speeches manually and also compare it to another package especially famous in the Python community, SpaCy (Benoit and Matsuo 2023).
```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
###############################################################
###          THIS NEEDS AT LEAST 2 HOURS to RUN!!!          ### 
###############################################################

# Download English model - enough to do once
# m_eng <- udpipe_download_model(language = "english-ewt")

# Load model from path
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")

# Lemmatize and POS tag speeches - BEWARE: code runs for at least 2 hours
pos <- udpipe_annotate(m_eng_loaded, x = df_clean$text) %>% 
  as.data.frame()

# Save data frame for later use
save(pos, file = "pos.RData")
```
We validate the performance of the lemmatizer and POS tagger by taking a random sample of speeches, conducting lemmatization and POS tagging manually on each token by ourselves and comparing them with those obtained by the pre-trained models of UDPipe and SpaCy. We coded a total of 1400 tokens manually, 98.5% of lemmas matched with the lemmas created by UDPipe and 99.1% of the POS tags matched with those from UDPipe. These results are as good as what we can expect from any model, leading us to be satisfied with UDPipe's pre-trained model. SpaCy palso roduces similarly strong results with differences being mostly irrelevant (e.g., one model lemmatizing 'an' as 'a' while the other keeps it as 'an', etc.). What made us choose UDPipe over SpaCy is that the latter splits compound words separated by hyphens into multiple lemmas, whereas UDPipe does not - While one could argue that splitting is the correct approach, such words are treated as one lemma in the sentiment lexicon (e.g. co-operation, high-quality, etc.) which means we could not assign a sentiment to these words if we used SpaCy's model.
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Load the POS tagged data and model
load("pos.RData")
m_eng_loaded <- udpipe_load_model(file = "english-ewt-ud-2.5-191206.udpipe")

# Sample 100 speeches
set.seed(123)    #For reproducibility
sampled_speeches <- df_clean %>% sample_n(100)

# Lemmatize and POS tag with UDPipe
pos_validation_UDPipe <- udpipe_annotate(m_eng_loaded, x = sampled_speeches$text) %>%
  as.data.frame()

# Lemmatize and POS tag with Spacyr
spacy_initialize(model = "en_core_web_sm")
pos_validation_spacy <- spacy_parse(sampled_speeches$text, lemma = TRUE, pos = TRUE, tag = TRUE)

# Save files for easier manual validation in Excel
pos_validation_UDPipe %>% write_xlsx("pos_validation_UDPipe.xlsx")
pos_validation_spacy %>% write_xlsx("pos_validation_spacy.xlsx")
```
As the following lines shows, the POS tags form UDPipe are different from those in the sentiment dictionary. In the next step, we need to create a mapping between them.
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
unique(pos$upos)
unique(sentiment_lexicon$pos1)
```
Based on the information in the table below, we map the POS tags in the sentiment lexicon to the tags in UDPipe, so that later we can match the sentiments based on them to our data.
```{r, echo=FALSE}
# Mapping table
maptable <- data.frame(
  "UDPipe POS" = c("NOUN", "VERB", "PUNCT", "CCONJ", "NUM", "SCONJ", "DET", 
                 "ADP", "ADJ", "PROPN", "PART", "ADV", "AUX", "PRON", 
                 "INTJ", "SYM", "X"),
  "Description" = c("Noun", "Verb", "Punctuation", "Coordinating conjunction", "Numeral",
                    "Subordinating conjunction", "Determiner", "Adposition", "Adjective",
                    "Proper noun", "Particle", "Adverb", "Auxiliary", "Pronoun",
                    "Interjection", "Symbol", "Other"),
  "Example tokens" = c("team, access, support", "commend, welcome, provide", ". , ()",
                       "and, or, but", "2020, two", "that, while, if, as", "a, the, what",
                       "in, to, by, with", "delighted, warm, flexible", "Member, House, European",
                       "'s, not, to", "certainly, very, preferably", "can, have, am",
                       "I, he, she, anyone", "yes, no, psst", "%, $, £", ""),
  "Sentiment lexicon POS" = c("n", "v", "", "", "", "", "",
                                  "", "a", "", "", "r", "", "",
                                  "u", "", ""),
  "Action" = c("change POS in lexicon: n --> NOUN", "change POS in lexicon: v --> VERB", "", "",
               "", "", "", "", "change POS in lexicon: a --> ADJ", "", "",
               "change POS in lexicon: r --> ADV", "", "", "change POS in lexicon: u --> INTJ",
               "", ""),
  check.names = FALSE
)

knitr::kable(maptable, caption = "Mapping guidance for POS tags")
```

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Mapping POS tags
sentiment_lexicon %>%
  mutate(pos1 = case_when(pos1 == 'n' ~ 'NOUN',
                          pos1 == 'v' ~ 'VERB',
                          pos1 == 'a' ~ 'ADJ',
                          pos1 == 'r' ~ 'ADV',
                          pos1 == 'u' ~ 'INTJ')) -> sentiment_lexicon
```
We can now match sentiments from the dictionary to the words in the speeches. Following the paper of Rheault et al. (2016), the sentiment in the House of Commons at a given time or over a time period is calculated in the following way. We take the sum of all sentiment scores associated with lemmas that occured in both the speeches and the lexicon, divide this by the total number of these lemmas and multiply the result with a factor called valence.


Now we can match the sentiments from the dictionary to the words in the speeches. And calculate the sentiment of each speech following the method described in the paper.
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Create a lookup table that store the sentiments of each speech
negating_words <- c("not", "no", "never", "neither", "nor")
pos %>%
  # Last 6 columns not needed
  select(-tail(names(.), 6)) %>%
  # Create 'valence' column as defined in the paper
      # Negation and punctuation indicator column
      mutate(is_negation = lemma %in% negating_words,
             is_punctuation = upos == "PUNCT") %>%
      # Running flag to identify regions between punctuations and regions between negating words
      mutate(punctuation_block = cumsum(is_punctuation),
             negation_block = cumsum(is_negation)) %>%
      # If there is a switch in negation block inside the same punctuation block we assign valence
      group_by(punctuation_block, negation_block) %>%
      mutate(valence = ifelse(any(is_negation), 0, 1)) %>%
      ungroup() %>%
      # Remove temporary auxiliary columns
      select(-is_negation, -is_punctuation, -negation_block, -negation_block) %>% 
  # Keep relevant POS tags only
  filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>%
  # Map polarity values
  left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
  # Calculate sentiment based on paper
  mutate(sentiment = polarity*valence) %>% 
  # Calculate sum of sentiment for each speech and number of words that had a sentiment
  group_by(doc_id) %>% 
    summarise(sentiment_sum = sum(replace_na(sentiment, 0)),
              sentiment_count = sum(!is.na(sentiment) & sentiment != 0)) -> sentiments_by_speech

# Map the sentiments to the speeches in the original data df_clean
df_clean %>% 
  # Add the doc_id column to df_clean to join on it in the subsequent step
  mutate(doc_id = paste0('doc', row_number())) %>% 
  left_join(sentiments_by_speech, by = 'doc_id') %>%
  # Replace NA values with 0 in the sentiment_sum and sentiment_count columns
  mutate(across(c(sentiment_sum, sentiment_count), ~ replace_na(.x, 0))) -> df_clean
```
Validation - take again the sample of 100 speeches, manually validate all words and their sentiment whether we think it's correct given the context of the whole speech/sentence.
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
pos_validation_UDPipe %>%
  # Last 6 columns not needed
  select(-tail(names(.), 6)) %>%
  # Create 'valence' column as defined in the paper
  mutate(valence = ifelse(lag(lemma) %in% negating_words & lead(upos) == "PUNCT", 0, 1)) %>%
  # Keep relevant POS tags only
  filter(upos %in% c('NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ')) %>% 
  # Map polarity values
  left_join(sentiment_lexicon, by = c('lemma' = 'lemma', 'upos' = 'pos1')) %>%
  # Calculate sentiment based on paper
  mutate(sentiment = polarity*valence) -> sentiment_validation

# Save file for easier manual validation in Excel
sentiment_validation %>% write_xlsx("sentiment_validation.xlsx")
```

Results/visualizations
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Visualize as in the paper
mblue <- "#002A48"
pblue <- "#3F5F75"

# Visualize the sentiment by date
df_clean %>% 
  group_by(debate_date) %>% 
  summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
                                       sum(sentiment_sum)/sum(sentiment_count))) %>%
  mutate(sentismooth = smooth.spline(average_sentiment, spar = 0.5)$y) %>%
  ggplot(aes(x = debate_date)) +
  geom_line(aes(y = average_sentiment), size = .1, colour=pblue) +
  geom_point(aes(y = average_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
  geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
  xlab("Date") +
  ylab('Sentiment') +
  theme_minimal()

# Visualize the sentiment by date - standardized data, as in paper
df_clean %>% 
  group_by(debate_date) %>% 
  summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
                                       sum(sentiment_sum)/sum(sentiment_count))) %>%
  mutate(standardized_sentiment = scale(average_sentiment),
         sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>%
  ggplot(aes(x = debate_date)) +
  geom_line(aes(y = standardized_sentiment), size = .1, colour=pblue) +
  geom_point(aes(y = standardized_sentiment), size = 6, colour=pblue, shape=1, stroke=.25) +
  geom_line(aes(y = sentismooth), size = 2, colour=mblue) +
  xlab("Date") +
  ylab('Sentiment') +
  # scale_y_continuous(limits = c(-3,3)) +
  theme_minimal()
  
# Visualize the sentiment by quarter - standardized data, as in paper
df_clean %>% 
  mutate(quarter = paste0(year(debate_date), "-Q", quarter(debate_date))) %>% 
  group_by(quarter) %>%
  summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
                                       sum(sentiment_sum) / sum(sentiment_count))) %>%
  mutate(standardized_sentiment = scale(average_sentiment), 
         sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>% 
  mutate(quarter = factor(quarter, levels = unique(quarter))) %>% 
  ggplot(aes(x = quarter, group = 1)) +
  geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
  geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
  geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
  xlab("Quarter") +
  ylab('Sentiment') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Visualize the sentiment by year - standardized data, as in paper
df_clean %>% 
  group_by(year) %>%
  summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
                                       sum(sentiment_sum) / sum(sentiment_count))) %>%
  mutate(standardized_sentiment = scale(average_sentiment), 
         sentismooth = smooth.spline(standardized_sentiment, spar = 0.5)$y) %>% 
  ggplot(aes(x = year, group = 1)) +
  geom_line(aes(y = standardized_sentiment), size = .1, colour = pblue) +
  geom_point(aes(y = standardized_sentiment), size = 6, colour = pblue, shape = 1, stroke = .25) +
  geom_line(aes(y = sentismooth), size = 2, colour = mblue) +
  xlab("Date") +
  ylab('Sentiment') +
  scale_x_continuous(breaks=c(2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022))  +
  theme_minimal()

```
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# Plot by Government-Opposition sentiment
df_clean %>% 
  mutate(quarter = paste0(year(debate_date), "-Q", quarter(debate_date))) %>% 
  group_by(quarter, Government_Opposition) %>%
  summarise(average_sentiment = ifelse(sum(sentiment_count) == 0, 0,
                                       sum(sentiment_sum)/sum(sentiment_count))) %>%
  pivot_wider(names_from = Government_Opposition, values_from = average_sentiment) %>% 
  select(-'NA') %>%
  ungroup() %>%
  mutate(sentismooth_government = smooth.spline(Government, spar = 0.5)$y,
         sentismooth_opposition = smooth.spline(Opposition, spar = 0.5)$y) %>%
  mutate(quarter = factor(quarter, levels = unique(quarter))) %>% 
  ggplot(aes(x = quarter, group = 1)) +
  geom_line(aes(y = Government), size = .1, colour="#3F5F75") +
  geom_point(aes(y = Government), size = 6, colour="#3F5F75", shape=1, stroke=.25) +
  geom_line(aes(y = Opposition), size = .1, colour="#3FA8C3") +
  geom_point(aes(y = Opposition), size = 6, colour="#3FA8C3", shape=1, stroke=.25) +
  geom_line(aes(y = sentismooth_government, colour="Government"), size = 2) +
  geom_line(aes(y = sentismooth_opposition, colour="Opposition"), size = 2) +
  scale_colour_manual(name="", values=c(Government="#002A48",Opposition="#008BB0")) +
  xlab("Quarter") +
  ylab('Sentiment') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position=c(0.4,0.2),
        legend.text = element_text(size = 10),
        legend.key.height=unit(1.5,"line"),
        legend.key.size=unit(2.5,"line"),
        legend.key = element_blank())
```

## References

Benoit, K., & Matsuo, A. (2023). spacyr: Wrapper to the 'spaCy' 'NLP' Library. https://CRAN.R-project.org/package=spacyr

House of Commons Library. (2016). *Analysis of the EU Referendum results 2016*. <https://commonslibrary.parliament.uk/research-briefings/cbp-7639/>. Accessed 21.08.2024

Public Administration and Constitutional Affairs Committee. (2017). Lessons Learned from
the EU Referendum (Twelth Report of Session 2016-17). *House of Commons*.

Rheault, L., Beelen, K., Cochrane, C., & Hirst, G. (2016). Measuring emotion in parliamentary debates with automated textual analysis. *PLoS ONE*, 11(12). https://doi.org/10.1371/journal.pone.0168843

Russell, M. (2021). Brexit and Parliament: The Anatomy of a Perfect Storm. *Parliamentary Affairs*, 74, 443-463  

UK Parliament. *Parliamentary Sovereignty*. <https://www.parliament.uk/site-information/glossary/parliamentary-sovereignty/>. Accessed 21.08.2024

Wijffels, Jan. 2021. Udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the ’UDPipe’ ’NLP’ Toolkit. https://CRAN.R-project.org/package=udpipe.
