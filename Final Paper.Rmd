---
title: "Britain_on_Europe"
output:
  html_document:
    df_print: paged
  
date: "`r Sys.Date()`"
---

# The EU from a British perspective: 17 years of debates in the British parliament

## Introduction
The positive outcome of the British referendum to withdrawal from the EU in 2016 came as a surprise to most other member states of the European Union.With a wining margin of only 3.8% and an overall approval to leave the European Union by 51.9% the decision was conceivably close (House of Commons Library, 2016). This direct democratic approach was relatively unusual especially since the United Kingdom of England and Northern Ireland (UK) has a strong representative democracy in which the sovereignty of the parliament is "the most important part of the UK constitution" (UK Parliament)^[The parliament generally refers to the house of commons. Thus in the following text house of commons and parliament are used interchangeably]. Furthermore, the Prime Minister at the time David Cameron as well as the biggest opposition party very much favoured remaining in the EU and thus disagreeing with the outcome of the referendum (cf. p.445, Russell, 2020). This vast majority for remaining in the EU in the parliament did not only lead to "no proper planning for a Leave vote" (Public Administration and Constitutional Affairs Committee, 2017, p. 4), but also to a clear difference between what most members of parliament (MP) and what the population wanted. The struggle of how to leave the EU thus triggered nearly 4 years of debates in the house of commons.

Russell (2020) raises that these debates which overturned deals with the EU multiple times even lead to an anti-parliamentary rhetoric from the Prime Ministers Theresa May as well as Boris Johnson and he Attorney General Geoffrey Cox.

This anti-parliamentary populist rhetoric raises the question whether the UK parliament really had a strong positive sentiment towards the EU and even more interesting if that was especially strong during the times of the Brexit debates.

This clearly shows that the European Union and especially Brexit is a highly emotional topic in British politics. 

- Emotion shape plitics
- Rheault: Measuring Emotion in Parliamentary Debates with Automated Textual Analysis, shows that debates became more positive.

Thus the following paper tries to find answers to the following research question: 

* How did the sentiment in debates about EU topics changed over time in the house of commons?

* Is the sentiment significantly different during debates about Brexit or withdrawl from the EU?

* Is there a visible difference depending on the current majorities in parliament?

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data
To answer the question we need the data of the debates in the british parliament. To gather these parliamentary debates, we web scraped the the debates in the house of commons from the TheyWorkForYou website. TheyWorkForYou is run by mySociety a british non for profit organization and aims to make the work of the Uk parliament more accesible and transparent to the wider public (source). In this capacity they also publish a mirrored version of Hansard the official webpage for transcripts of UK debates (source). 

In our work we use TheyWorkForYou instead of Hansard as the content is exactly the same but Hansard uses CloudFlare which makes web scraping significantly more complicated.

### Data collection
The data we gather include all debates between 27.06.2007 until the 19.08.2024. The starting date is chosen as it is the first day of the Brown ministry the first government that had the possibility to retreat from the EU due to Article 50 (source) (more detailed explanation). The data is constructed by searching for all debates in the time frame that mention the European Union, European Commission, European Council, or Brexit. These debates are then filtered to only include debates that have European Union, European Commission, European Council, Brexit, European, and Europe in their title and are no written answers. Furthermore the date of the debate is saved as well as the name and party/position of the speakers in the debate are saved. Thus, resulting in a data frame representing 13,831 different debates.

```{r, eval=FALSE, echo=TRUE}
# Webscraping

###############################################################
###          THIS NEEDS AT LEAST 5 HOURS to RUN!!!          ### 
###############################################################

# install necesary packages

install.packages(c("tidyverse", 
                   "httr", "glue", "dplyr", "stringr"))
library(dplyr)
library("tidyverse")
library("httr")
library("rvest")
library("xml2")
library("glue")
library("rvest")
library("xml2")
library("glue")
library(stringr)
SearchTerm <- "(\"EU\"+OR+\"European+Commission\"+OR+\"European+Union\"+OR+\"European+Council\"+OR+\"Brexit\")"
TitleTerms <- c("European Union","European Commission","European Council", "Brexit")

StartDate <- c("2012-10-25", "2012-12-25", "2013-02-25", "2013-04-25", 
               "2013-06-25", "2013-08-25", "2013-10-25", "2013-12-25", 
               "2014-02-25", "2014-04-25", "2014-06-25", "2014-08-25", 
               "2014-10-25", "2014-12-25", "2015-02-25", "2015-04-25", 
               "2015-06-25", "2015-08-25", "2015-10-25", "2015-12-25", 
               "2016-02-25", "2016-04-25", "2016-06-25", "2016-08-25", 
               "2016-10-25", "2016-12-25", "2017-02-25", "2017-04-25", 
               "2017-06-25", "2017-08-25", "2017-10-25", "2017-12-25", 
               "2018-02-25", "2018-04-25", "2018-06-25", "2018-08-25", 
               "2018-10-25", "2018-12-25", "2019-02-25", "2019-04-25", 
               "2019-06-25", "2019-08-25", "2019-10-25", "2019-12-25", 
               "2020-02-25", "2020-04-25", "2020-06-25", "2020-08-25", 
               "2020-10-25", "2020-12-25", "2021-02-25", "2021-04-25", 
               "2021-06-25", "2021-08-25", "2021-10-25", "2021-12-25", 
               "2022-02-25", "2022-04-25", "2022-06-25", "2022-08-25", 
               "2022-10-25")

EndDate <- c("2012-12-25", "2013-02-25", "2013-04-25", "2013-06-25", 
             "2013-08-25", "2013-10-25", "2013-12-25", "2014-02-25", 
             "2014-04-25", "2014-06-25", "2014-08-25", "2014-10-25", 
             "2014-12-25", "2015-02-25", "2015-04-25", "2015-06-25", 
             "2015-08-25", "2015-10-25", "2015-12-25", "2016-02-25", 
             "2016-04-25", "2016-06-25", "2016-08-25", "2016-10-25", 
             "2016-12-25", "2017-02-25", "2017-04-25", "2017-06-25", 
             "2017-08-25", "2017-10-25", "2017-12-25", "2018-02-25", 
             "2018-04-25", "2018-06-25", "2018-08-25", "2018-10-25", 
             "2018-12-25", "2019-02-25", "2019-04-25", "2019-06-25", 
             "2019-08-25", "2019-10-25", "2019-12-25", "2020-02-25", 
             "2020-04-25", "2020-06-25", "2020-08-25", "2020-10-25", 
             "2020-12-25", "2021-02-25", "2021-04-25", "2021-06-25", 
             "2021-08-25", "2021-10-25", "2021-12-25", "2022-02-25", 
             "2022-04-25", "2022-06-25", "2022-08-25", "2022-10-25", 
             "2022-12-25")
base_url = "https://www.theyworkforyou.com"

for(i in 1:length(StartDate)){
  # Initialize a data frame to store the results
  results_df <- data.frame(
    speech_id = character(),
    debate_date = character(),
    debate_title = character (),
    speech_content = character(),
    speaker_name = character(),
    speaker_party = character(),
    stringsAsFactors = FALSE
  )
  
  # Initialize vectors to store all titles and links
  all_titles <- c()
  all_links <- c()
    
  link <- paste0(base_url, "/search/?q=%28%22EU%22+OR+%22European+Commission%22+OR+%22European+Union%22+OR+%22European+Council%22+OR+%22Brexit%22%29+",StartDate[i],"..",EndDate[i],"+section%3Adebates+section%3Awhall&p=1")
    
  # Error handling for the main page
  page <- tryCatch({
    read_html(link)
  }, error = function(e) {
    message("Error accessing main search page: ", link)
    return(NULL)
  })
    
  # Extract titles and links from the first page
  titles <- page %>%
    html_nodes(".search-result__title a") %>%  # Adjust the CSS selector if needed
    html_text()
  
  links <- page %>%
    html_nodes(".search-result__title a") %>%  # Adjust the CSS selector if needed
    html_attr("href")
    
  # Filter links that start with "/debates"
  debate_links <- links[grep("^/debates", links)]
  
  clean_link <- gsub("#.*$", "", links)
    
  # Combine them with all_titles and all_links
  all_titles <- c(all_titles, titles)
  all_links <- c(all_links, clean_link)

  # Get the final page number
  final_page_number <- page %>%
    html_nodes(".search-result-pagination a[title='Final page']") %>% 
    html_attr("href") %>% 
    str_extract("p=\\d+") %>% 
    str_remove("p=") %>% 
    as.numeric()
    
  # Loop through all subsequent pages and extract titles and links
  if (!is.na(final_page_number) && length(final_page_number) > 0&& final_page_number > 1) {
    for (j in 2:final_page_number) {
      next_link <- paste0(base_url, "/search/?q=%28%22EU%22+OR+%22European+Commission%22+OR+%22European+Union%22+OR+%22European+Council%22+OR+%22Brexit%22%29+",StartDate[i],"..",EndDate[i],"+section%3Adebates+section%3Awhall&p=", j)
        
      #Sys.sleep(sample(3:4, 1))  # Random delay between 3 to 5 seconds
        
      # Error handling for subsequent pages
      next_page <- tryCatch({
        read_html(next_link)
      }, error = function(e) {
        message("Error accessing subsequent search page: ", next_link)
        return(NULL)
      })
      
      if (is.null(next_page)) next  # Skip to the next iteration if the page is NULL
        
      # Extract titles and links from the current page
      titles <- next_page %>%
        html_nodes(".search-result__title a") %>%
        html_text()
        
      links <- next_page %>%
        html_nodes(".search-result__title a") %>%
        html_attr("href")
      
      # Filter links that start with "/debates"
      debate_links <- links[grep("^/debates", links)]
        
      clean_link <- gsub("#.*$", "", links)
        
      # Combine them with all_titles and all_links
      all_titles <- c(all_titles, titles)
      all_links <- c(all_links, clean_link)
    }
  } else {
    print("Only one page exists or final page number not found.")
  }
    
  # Filter links to only include ones with European, Europe, and EU in the title
  # which are not Written answers.
  filtered_links <- all_links[
    grepl(paste("European", "Europe", "EU", sep="|"), all_titles, ignore.case = TRUE) & 
      !grepl("Written Answer", all_titles, ignore.case = TRUE)
  ]
  
    for (link_debate in filtered_links){
      #Sys.sleep(sample(3:4,1))  # Random delay between 3 to 5 seconds
      
      # Find debate pages
      debate_page <- tryCatch({
        read_html(paste0(base_url,link_debate))
      }, error = function(e) {
        message("Error accessing debate page: ", link_debate)
        return(NULL)
      })
        
      if (is.null(debate_page)) next  # Skip to the next iteration if the page is NULL
      
      # Extract debate date
      debate_date <- debate_page %>%
        html_node("p.lead a") %>%
        html_text(trim = TRUE)
        
      # Extract the debate title
      debate_title <- debate_page %>%
        html_node("div.debate-header__content h1") %>%
        html_text(trim = TRUE)
        
      # Extract debate speeches
      speeches <- debate_page %>%
        html_nodes(".debate-speech")
        
      # Loop through each speech node to extract the required information
      for (speech in speeches) {
        # Get the speech ID
        speech_id <- speech %>% 
          html_attr("id")
        
        # Get the speaker's name
        speaker_name <- speech %>%
          html_node(".debate-speech__speaker .debate-speech__speaker__name") %>%
          html_text(trim = TRUE)
        
        # Get the speaker's position (if it exists)
        speaker_party <- speech %>%
          html_node(".debate-speech__speaker .debate-speech__speaker__position") %>%
          html_text(trim = TRUE)
          
        # Get the speech content
        speech_content <- speech %>%
          html_node(".debate-speech__content") %>%
          html_text(trim = TRUE)
          
        results_df <- results_df %>%
          add_row(
            speech_id = speech_id,  # Character type
            debate_date = debate_date,  # Character type
            debate_title = debate_title,  # Character type
            speech_content = speech_content,  # Character type
            speaker_name = speaker_name,  # Character type
            speaker_party = speaker_party  # Character type
          )
      }
    } 
  
  output_file <- file.path("C:/Users/leonw/OneDrive - KU Leuven/2nd Semester/Collecting Big Data for Social Science/Final Assignment", paste0("Parliamentary_Debates_", i, ".csv"))
  write.csv(results_df, output_file, row.names = FALSE)
  Sys.sleep(30)
}

csv_files <- list.files(pattern = "^Parliamentary_Debates_.*\\.csv$")

list_of_data_frames <- lapply(csv_files, read.csv)

# Step 4: Combine all data frames into one using do.call and rbind
combined_data_frame <- do.call(rbind, list_of_data_frames)

combined_data_frame$year <- as.integer(substr(combined_data_frame$debate_date, nchar(combined_data_frame$debate_date) - 4 + 1, nchar(combined_data_frame$debate_date)))

n_distinct(combined_data_frame$year)

df_clean <- distinct(combined_data_frame)

write.csv(df_clean, "C:/Users/leonw/OneDrive - KU Leuven/2nd Semester/Collecting Big Data for Social Science/Final Assignment/Parliamentary_Debates_Combined.csv")
```

### Ethical considerations
Web scrapping websites always comes with certain ethical considerations. Checking the robots.txt of the websites shows that it is legal to scrape the debates in the house of commons. Despite the legal considerations sustainable approaches were used to scrape the data. First of all scraping was conducted during the night to avoid overloading the page in times when it is frequently used. Second a waiting time between 3 and 4 seconds was chosen before scraping to ensure not to burden the webpage to much.

## Data cleaning
```{r, message=FALSE}

library("tidyverse")
library("httr")
library("jsonlite")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("rvest")
library("xml2")
library("glue")
library("data.table")
library("quanteda")
library("topicmodels")
library(readxl)
library(tm)
library(tidytext)
library(dplyr)
library(lubridate)    # for date conversion in data cleaning
```

```{r}
df_result = read.csv('Parliamentary_Debates.csv', row.names = 1)

# Delete all the duplicates
df_clean <- distinct(df_result)

n_distinct(df_clean$speech_id)
n_distinct(df_clean$speech_content)

# Format the debate_date column so that it contains dates not character strings
df_clean$debate_date <- dmy(df_clean$debate_date)
```


## Sentiments
```{r}
poswords = "https://cssbook.net/d/positive.txt"
negwords = "https://cssbook.net/d/negative.txt"

pos = scan(poswords, what = "list")
neg = scan(negwords, what = "list")

sentimentdict = dictionary(list(pos = pos, neg = neg))

# Need to rename the column of the speeches to 'text' because the corpus() function from the following code needs that naming convention
names(df_clean)[names(df_clean) == "speech_content"] <- "text"

# Match and calculate the sentiments
sentiment_speech = df_clean %>% 
  corpus() %>% 
  tokens() %>% 
  dfm() %>% 
  dfm_lookup(sentimentdict) %>% 
  convert(to = "data.frame") %>%
  mutate(sent = pos-neg)

df_clean <- cbind(df_clean, sentiment_speech[,-1])

df_clean %>% 
  group_by(debate_title) %>% 
  summarise(mean_sent = mean(sent)) %>%
  ggplot(aes(x = mean_sent, y = debate_title)) +
  geom_bar(stat = "identity", fill = 'steelblue') +
  theme_minimal()

# This is probably incorrect, would need grouping like before imo
ggplot(df_clean, aes(x = debate_date, y = sent)) +
  geom_line(color='steelblue') +
  xlab("Year") +
  theme_minimal()
```
## References

House of Commons Library. (2016). *Analysis of the EU Referendum results 2016*. <https://commonslibrary.parliament.uk/research-briefings/cbp-7639/>. Accessed 21.08.2024

Public Administration and Constitutional Affairs Committee. (2017). Lessons Learned from
the EU Referendum (Twelth Report of Session 2016-17). *House of Commons*.

Russell, M. (2021). Brexit and Parliament: The Anatomy of a Perfect Storm. *Parliamentary Affairs*, 74, 443-463  

UK Parliament. *Parliamentary Sovereignty*. <https://www.parliament.uk/site-information/glossary/parliamentary-sovereignty/>. Accessed 21.08.2024
